{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Training SegFormer cho Ph√¢n ƒëo·∫°n Kh·ªëi u Da\n",
    "\n",
    "Notebook n√†y th·ª±c hi·ªán training model **SegFormer** cho b√†i to√°n ph√¢n ƒëo·∫°n kh·ªëi u da:\n",
    "\n",
    "## üéØ Model: SegFormer\n",
    "- **Architecture**: Transformer-based segmentation\n",
    "- **Backbone**: `nvidia/segformer-b0-finetuned-ade-512-512`\n",
    "- **Parameters**: ~3.7M (r·∫•t nh·∫π)\n",
    "- **∆Øu ƒëi·ªÉm**: Hi·ªáu su·∫•t cao, √≠t tham s·ªë, x·ª≠ l√Ω t·ªët v·ªõi medical images\n",
    "- **Learning Rate Scheduler**: Cosine Annealing (ph√π h·ª£p v·ªõi transformer)\n",
    "\n",
    "## üìä Training Configuration:\n",
    "- **Epochs**: 20\n",
    "- **Learning Rate**: 5e-5 (th·∫•p h∆°n cho pre-trained transformer)\n",
    "- **Batch Size**: 8\n",
    "- **Optimizer**: Adam v·ªõi weight decay 1e-4\n",
    "- **Loss Function**: Combined Loss (BCE + Dice)\n",
    "- **Metrics**: Dice Coefficient, Jaccard Index (IoU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import th∆∞ vi·ªán v√† setup\n",
    "\n",
    "### üö® Quan tr·ªçng: C√†i ƒë·∫∑t GPU Support\n",
    "\n",
    "ƒê·ªÉ s·ª≠ d·ª•ng GPU cho training, b·∫°n c·∫ßn:\n",
    "\n",
    "1. **Ki·ªÉm tra GPU**: Ch·∫°y `nvidia-smi` trong terminal\n",
    "2. **C√†i CUDA Toolkit**: T·∫£i t·ª´ [NVIDIA CUDA](https://developer.nvidia.com/cuda-downloads)\n",
    "3. **C√†i PyTorch v·ªõi CUDA**: \n",
    "   ```bash\n",
    "   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "   ```\n",
    "   (Thay `cu118` b·∫±ng phi√™n b·∫£n CUDA c·ªßa b·∫°n: cu117, cu118, cu121)\n",
    "\n",
    "4. **Ki·ªÉm tra**: Ch·∫°y cell d∆∞·ªõi ƒë·ªÉ verify GPU ho·∫°t ƒë·ªông"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.55.0)\n",
      "Collecting timm\n",
      "  Downloading timm-1.0.19-py3-none-any.whl.metadata (60 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.7.1)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.23.0-cp311-cp311-win_amd64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.6.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.1.6)\n",
      "Collecting torch\n",
      "  Downloading torch-2.8.0-cp311-cp311-win_amd64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Downloading timm-1.0.19-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.5 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 1.0/2.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.8/2.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 3.3 MB/s  0:00:00\n",
      "Downloading torchvision-0.23.0-cp311-cp311-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 1.0/1.6 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 4.5 MB/s  0:00:00\n",
      "Downloading torch-2.8.0-cp311-cp311-win_amd64.whl (241.4 MB)\n",
      "   ---------------------------------------- 0.0/241.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/241.4 MB 6.3 MB/s eta 0:00:39\n",
      "   ---------------------------------------- 2.4/241.4 MB 6.1 MB/s eta 0:00:40\n",
      "    --------------------------------------- 3.7/241.4 MB 6.1 MB/s eta 0:00:40\n",
      "    --------------------------------------- 4.7/241.4 MB 6.1 MB/s eta 0:00:39\n",
      "    --------------------------------------- 6.0/241.4 MB 6.1 MB/s eta 0:00:39\n",
      "   - -------------------------------------- 7.3/241.4 MB 6.1 MB/s eta 0:00:39\n",
      "   - -------------------------------------- 8.7/241.4 MB 6.1 MB/s eta 0:00:39\n",
      "   - -------------------------------------- 10.0/241.4 MB 6.1 MB/s eta 0:00:39\n",
      "   - -------------------------------------- 11.0/241.4 MB 6.1 MB/s eta 0:00:38\n",
      "   -- ------------------------------------- 12.3/241.4 MB 6.1 MB/s eta 0:00:38\n",
      "   -- ------------------------------------- 13.6/241.4 MB 6.1 MB/s eta 0:00:38\n",
      "   -- ------------------------------------- 14.9/241.4 MB 6.1 MB/s eta 0:00:38\n",
      "   -- ------------------------------------- 16.0/241.4 MB 6.1 MB/s eta 0:00:37\n",
      "   -- ------------------------------------- 17.3/241.4 MB 6.1 MB/s eta 0:00:37\n",
      "   --- ------------------------------------ 18.6/241.4 MB 6.1 MB/s eta 0:00:37\n",
      "   --- ------------------------------------ 19.7/241.4 MB 6.1 MB/s eta 0:00:37\n",
      "   --- ------------------------------------ 21.0/241.4 MB 6.1 MB/s eta 0:00:37\n",
      "   --- ------------------------------------ 22.3/241.4 MB 6.1 MB/s eta 0:00:37\n",
      "   --- ------------------------------------ 23.6/241.4 MB 6.1 MB/s eta 0:00:36\n",
      "   ---- ----------------------------------- 24.9/241.4 MB 6.1 MB/s eta 0:00:36\n",
      "   ---- ----------------------------------- 26.0/241.4 MB 6.1 MB/s eta 0:00:36\n",
      "   ---- ----------------------------------- 27.5/241.4 MB 6.1 MB/s eta 0:00:36\n",
      "   ---- ----------------------------------- 28.6/241.4 MB 6.1 MB/s eta 0:00:35\n",
      "   ---- ----------------------------------- 29.6/241.4 MB 6.0 MB/s eta 0:00:36\n",
      "   ----- ---------------------------------- 30.9/241.4 MB 6.1 MB/s eta 0:00:35\n",
      "   ----- ---------------------------------- 32.0/241.4 MB 6.0 MB/s eta 0:00:35\n",
      "   ----- ---------------------------------- 33.0/241.4 MB 6.0 MB/s eta 0:00:35\n",
      "   ----- ---------------------------------- 33.8/241.4 MB 5.9 MB/s eta 0:00:35\n",
      "   ----- ---------------------------------- 34.6/241.4 MB 5.9 MB/s eta 0:00:36\n",
      "   ----- ---------------------------------- 35.4/241.4 MB 5.8 MB/s eta 0:00:36\n",
      "   ----- ---------------------------------- 36.2/241.4 MB 5.7 MB/s eta 0:00:36\n",
      "   ------ --------------------------------- 37.0/241.4 MB 5.6 MB/s eta 0:00:37\n",
      "   ------ --------------------------------- 37.7/241.4 MB 5.6 MB/s eta 0:00:37\n",
      "   ------ --------------------------------- 38.3/241.4 MB 5.5 MB/s eta 0:00:37\n",
      "   ------ --------------------------------- 38.8/241.4 MB 5.5 MB/s eta 0:00:38\n",
      "   ------ --------------------------------- 39.3/241.4 MB 5.4 MB/s eta 0:00:38\n",
      "   ------ --------------------------------- 40.1/241.4 MB 5.3 MB/s eta 0:00:39\n",
      "   ------ --------------------------------- 40.6/241.4 MB 5.2 MB/s eta 0:00:39\n",
      "   ------ --------------------------------- 41.2/241.4 MB 5.2 MB/s eta 0:00:39\n",
      "   ------ --------------------------------- 41.9/241.4 MB 5.1 MB/s eta 0:00:40\n",
      "   ------- -------------------------------- 42.5/241.4 MB 5.0 MB/s eta 0:00:40\n",
      "   ------- -------------------------------- 43.3/241.4 MB 5.0 MB/s eta 0:00:40\n",
      "   ------- -------------------------------- 43.8/241.4 MB 5.0 MB/s eta 0:00:40\n",
      "   ------- -------------------------------- 44.6/241.4 MB 4.9 MB/s eta 0:00:41\n",
      "   ------- -------------------------------- 45.1/241.4 MB 4.9 MB/s eta 0:00:41\n",
      "   ------- -------------------------------- 45.9/241.4 MB 4.8 MB/s eta 0:00:41\n",
      "   ------- -------------------------------- 46.7/241.4 MB 4.8 MB/s eta 0:00:41\n",
      "   ------- -------------------------------- 47.2/241.4 MB 4.8 MB/s eta 0:00:41\n",
      "   ------- -------------------------------- 48.0/241.4 MB 4.8 MB/s eta 0:00:41\n",
      "   -------- ------------------------------- 48.8/241.4 MB 4.7 MB/s eta 0:00:41\n",
      "   -------- ------------------------------- 49.5/241.4 MB 4.7 MB/s eta 0:00:41\n",
      "   -------- ------------------------------- 50.3/241.4 MB 4.7 MB/s eta 0:00:41\n",
      "   -------- ------------------------------- 51.1/241.4 MB 4.7 MB/s eta 0:00:41\n",
      "   -------- ------------------------------- 51.9/241.4 MB 4.7 MB/s eta 0:00:41\n",
      "   -------- ------------------------------- 52.7/241.4 MB 4.7 MB/s eta 0:00:41\n",
      "   -------- ------------------------------- 53.7/241.4 MB 4.7 MB/s eta 0:00:41\n",
      "   --------- ------------------------------ 54.5/241.4 MB 4.6 MB/s eta 0:00:41\n",
      "   --------- ------------------------------ 55.3/241.4 MB 4.6 MB/s eta 0:00:41\n",
      "   --------- ------------------------------ 56.4/241.4 MB 4.6 MB/s eta 0:00:40\n",
      "   --------- ------------------------------ 57.1/241.4 MB 4.6 MB/s eta 0:00:40\n",
      "   --------- ------------------------------ 58.2/241.4 MB 4.6 MB/s eta 0:00:40\n",
      "   --------- ------------------------------ 59.0/241.4 MB 4.6 MB/s eta 0:00:40\n",
      "   --------- ------------------------------ 60.0/241.4 MB 4.6 MB/s eta 0:00:40\n",
      "   ---------- ----------------------------- 60.8/241.4 MB 4.6 MB/s eta 0:00:39\n",
      "   ---------- ----------------------------- 61.9/241.4 MB 4.6 MB/s eta 0:00:39\n",
      "   ---------- ----------------------------- 62.9/241.4 MB 4.6 MB/s eta 0:00:39\n",
      "   ---------- ----------------------------- 64.0/241.4 MB 4.6 MB/s eta 0:00:39\n",
      "   ---------- ----------------------------- 65.0/241.4 MB 4.6 MB/s eta 0:00:38\n",
      "   ---------- ----------------------------- 65.8/241.4 MB 4.7 MB/s eta 0:00:38\n",
      "   ----------- ---------------------------- 66.8/241.4 MB 4.7 MB/s eta 0:00:38\n",
      "   ----------- ---------------------------- 68.2/241.4 MB 4.7 MB/s eta 0:00:38\n",
      "   ----------- ---------------------------- 69.2/241.4 MB 4.7 MB/s eta 0:00:37\n",
      "   ----------- ---------------------------- 70.3/241.4 MB 4.7 MB/s eta 0:00:37\n",
      "   ----------- ---------------------------- 71.3/241.4 MB 4.7 MB/s eta 0:00:37\n",
      "   ------------ --------------------------- 72.6/241.4 MB 4.7 MB/s eta 0:00:36\n",
      "   ------------ --------------------------- 73.7/241.4 MB 4.7 MB/s eta 0:00:36\n",
      "   ------------ --------------------------- 74.7/241.4 MB 4.7 MB/s eta 0:00:36\n",
      "   ------------ --------------------------- 76.0/241.4 MB 4.7 MB/s eta 0:00:35\n",
      "   ------------ --------------------------- 77.1/241.4 MB 4.7 MB/s eta 0:00:35\n",
      "   ------------ --------------------------- 78.4/241.4 MB 4.8 MB/s eta 0:00:35\n",
      "   ------------- -------------------------- 79.7/241.4 MB 4.8 MB/s eta 0:00:34\n",
      "   ------------- -------------------------- 80.7/241.4 MB 4.8 MB/s eta 0:00:34\n",
      "   ------------- -------------------------- 82.1/241.4 MB 4.8 MB/s eta 0:00:34\n",
      "   ------------- -------------------------- 83.4/241.4 MB 4.8 MB/s eta 0:00:33\n",
      "   -------------- ------------------------- 84.9/241.4 MB 4.9 MB/s eta 0:00:33\n",
      "   -------------- ------------------------- 86.2/241.4 MB 4.9 MB/s eta 0:00:32\n",
      "   -------------- ------------------------- 87.8/241.4 MB 4.9 MB/s eta 0:00:32\n",
      "   -------------- ------------------------- 89.7/241.4 MB 5.0 MB/s eta 0:00:31\n",
      "   --------------- ------------------------ 91.2/241.4 MB 5.0 MB/s eta 0:00:31\n",
      "   --------------- ------------------------ 92.8/241.4 MB 5.0 MB/s eta 0:00:30\n",
      "   --------------- ------------------------ 94.6/241.4 MB 5.1 MB/s eta 0:00:29\n",
      "   ---------------- ----------------------- 96.7/241.4 MB 5.1 MB/s eta 0:00:29\n",
      "   ---------------- ----------------------- 98.8/241.4 MB 5.2 MB/s eta 0:00:28\n",
      "   ---------------- ----------------------- 101.2/241.4 MB 5.2 MB/s eta 0:00:27\n",
      "   ----------------- ---------------------- 103.5/241.4 MB 5.3 MB/s eta 0:00:26\n",
      "   ----------------- ---------------------- 105.9/241.4 MB 5.4 MB/s eta 0:00:26\n",
      "   ----------------- ---------------------- 108.5/241.4 MB 5.5 MB/s eta 0:00:25\n",
      "   ------------------ --------------------- 111.1/241.4 MB 5.5 MB/s eta 0:00:24\n",
      "   ------------------ --------------------- 114.3/241.4 MB 5.6 MB/s eta 0:00:23\n",
      "   ------------------- -------------------- 116.7/241.4 MB 5.7 MB/s eta 0:00:22\n",
      "   ------------------- -------------------- 118.5/241.4 MB 5.7 MB/s eta 0:00:22\n",
      "   -------------------- ------------------- 121.9/241.4 MB 5.8 MB/s eta 0:00:21\n",
      "   -------------------- ------------------- 125.3/241.4 MB 5.9 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 129.2/241.4 MB 6.1 MB/s eta 0:00:19\n",
      "   ---------------------- ----------------- 132.9/241.4 MB 6.2 MB/s eta 0:00:18\n",
      "   ---------------------- ----------------- 137.4/241.4 MB 6.3 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 141.6/241.4 MB 6.4 MB/s eta 0:00:16\n",
      "   ------------------------ --------------- 145.8/241.4 MB 6.6 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 150.5/241.4 MB 6.7 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 155.7/241.4 MB 6.9 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 160.7/241.4 MB 7.1 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 165.9/241.4 MB 7.2 MB/s eta 0:00:11\n",
      "   ---------------------------- ----------- 171.7/241.4 MB 7.4 MB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 177.7/241.4 MB 7.6 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 183.8/241.4 MB 7.8 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 190.1/241.4 MB 8.0 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 196.6/241.4 MB 8.2 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 203.9/241.4 MB 8.4 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 211.0/241.4 MB 8.6 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 218.4/241.4 MB 8.9 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 226.2/241.4 MB 9.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 234.4/241.4 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.4 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.4 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.4 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.4 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.4 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.4 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.4 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 241.4/241.4 MB 9.1 MB/s  0:00:26\n",
      "Installing collected packages: torch, torchvision, timm\n",
      "\n",
      "  Attempting uninstall: torch\n",
      "\n",
      "    Found existing installation: torch 2.7.1\n",
      "\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "    Uninstalling torch-2.7.1:\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "      Successfully uninstalled torch-2.7.1\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   -------------------------- ------------- 2/3 [timm]\n",
      "   -------------------------- ------------- 2/3 [timm]\n",
      "   -------------------------- ------------- 2/3 [timm]\n",
      "   -------------------------- ------------- 2/3 [timm]\n",
      "   -------------------------- ------------- 2/3 [timm]\n",
      "   -------------------------- ------------- 2/3 [timm]\n",
      "   -------------------------- ------------- 2/3 [timm]\n",
      "   -------------------------- ------------- 2/3 [timm]\n",
      "   -------------------------- ------------- 2/3 [timm]\n",
      "   ---------------------------------------- 3/3 [timm]\n",
      "\n",
      "Successfully installed timm-1.0.19 torch-2.8.0 torchvision-0.23.0\n",
      "Collecting albumentations\n",
      "  Downloading albumentations-2.0.8-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: opencv-python-headless in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy>=1.24.4 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from albumentations) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from albumentations) (1.16.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from albumentations) (2.11.7)\n",
      "Collecting albucore==0.0.24 (from albumentations)\n",
      "  Downloading albucore-0.0.24-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting stringzilla>=3.10.4 (from albucore==0.0.24->albumentations)\n",
      "  Downloading stringzilla-3.12.5-cp311-cp311-win_amd64.whl.metadata (81 kB)\n",
      "Collecting simsimd>=5.9.2 (from albucore==0.0.24->albumentations)\n",
      "  Downloading simsimd-6.5.0-cp311-cp311-win_amd64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic>=2.9.2->albumentations) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n",
      "Downloading albumentations-2.0.8-py3-none-any.whl (369 kB)\n",
      "Downloading albucore-0.0.24-py3-none-any.whl (15 kB)\n",
      "Downloading simsimd-6.5.0-cp311-cp311-win_amd64.whl (94 kB)\n",
      "Downloading stringzilla-3.12.5-cp311-cp311-win_amd64.whl (80 kB)\n",
      "Installing collected packages: stringzilla, simsimd, albucore, albumentations\n",
      "\n",
      "   ------------------------------ --------- 3/4 [albumentations]\n",
      "   ------------------------------ --------- 3/4 [albumentations]\n",
      "   ---------------------------------------- 4/4 [albumentations]\n",
      "\n",
      "Successfully installed albucore-0.0.24 albumentations-2.0.8 simsimd-6.5.0 stringzilla-3.12.5\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.10.5)\n",
      "Requirement already satisfied: seaborn in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.7.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (11.3.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from seaborn) (2.3.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "# C√†i ƒë·∫∑t PyTorch v·ªõi CUDA support (thay ƒë·ªïi cu118 theo phi√™n b·∫£n CUDA c·ªßa b·∫°n)\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers timm\n",
    "!pip install albumentations opencv-python-headless\n",
    "!pip install matplotlib seaborn scikit-learn pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cpu\n",
      "CUDA available: False\n",
      "‚ùå CUDA kh√¥ng kh·∫£ d·ª•ng. Ki·ªÉm tra:\n",
      "1. Driver GPU ƒë√£ c√†i ƒë·∫∑t ch∆∞a?\n",
      "2. CUDA toolkit ƒë√£ c√†i ƒë·∫∑t ch∆∞a?\n",
      "3. PyTorch c√≥ ƒë∆∞·ª£c c√†i v·ªõi CUDA support kh√¥ng?\n",
      "\n",
      "üí° ƒê·ªÉ c√†i PyTorch v·ªõi CUDA:\n",
      "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
     ]
    }
   ],
   "source": [
    "# Ki·ªÉm tra CUDA v√† GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA kh√¥ng kh·∫£ d·ª•ng. Ki·ªÉm tra:\")\n",
    "    print(\"1. Driver GPU ƒë√£ c√†i ƒë·∫∑t ch∆∞a?\")\n",
    "    print(\"2. CUDA toolkit ƒë√£ c√†i ƒë·∫∑t ch∆∞a?\")\n",
    "    print(\"3. PyTorch c√≥ ƒë∆∞·ª£c c√†i v·ªõi CUDA support kh√¥ng?\")\n",
    "    print(\"\\nüí° ƒê·ªÉ c√†i PyTorch v·ªõi CUDA:\")\n",
    "    print(\"pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Hugging Face Transformers\n",
    "from transformers import (\n",
    "    SegformerImageProcessor, SegformerForSemanticSegmentation,\n",
    "    AutoImageProcessor, AutoModelForSemanticSegmentation\n",
    ")\n",
    "\n",
    "# Albumentations for augmentation\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import jaccard_score, f1_score\n",
    "\n",
    "# Set device v·ªõi th√¥ng tin chi ti·∫øt\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üîß Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üìä Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  ƒêang s·ª≠ d·ª•ng CPU - training s·∫Ω ch·∫≠m h∆°n\")\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c models n·∫øu ch∆∞a c√≥\n",
    "os.makedirs('models', exist_ok=True)\n",
    "print(\"‚úÖ Setup ho√†n t·∫•t!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset v√† Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISICDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None, target_size=(512, 512)):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        # L·∫•y danh s√°ch file images\n",
    "        self.image_files = sorted([f for f in os.listdir(images_dir) \n",
    "                                 if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        \n",
    "        print(f\"Found {len(self.image_files)} images in {images_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Find corresponding mask\n",
    "        base_name = os.path.splitext(img_name)[0]\n",
    "        possible_mask_names = [\n",
    "            f\"{base_name}_segmentation.png\",\n",
    "            f\"{base_name}_mask.png\",\n",
    "            f\"{base_name}.png\"\n",
    "        ]\n",
    "        \n",
    "        mask = None\n",
    "        for mask_name in possible_mask_names:\n",
    "            mask_path = os.path.join(self.masks_dir, mask_name)\n",
    "            if os.path.exists(mask_path):\n",
    "                mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "                break\n",
    "        \n",
    "        if mask is None:\n",
    "            # Create dummy mask if not found\n",
    "            mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "        \n",
    "        # Resize\n",
    "        image = cv2.resize(image, self.target_size)\n",
    "        mask = cv2.resize(mask, self.target_size)\n",
    "        \n",
    "        # Convert mask to binary\n",
    "        mask = (mask > 127).astype(np.float32)\n",
    "        \n",
    "        # Apply augmentations\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        \n",
    "        # Convert to tensor if not already\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n",
    "        \n",
    "        if not isinstance(mask, torch.Tensor):\n",
    "            mask = torch.from_numpy(mask).float().unsqueeze(0)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# Define augmentations\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Dataset class v√† augmentations ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = ISICDataset(\n",
    "    images_dir='data/train/images',\n",
    "    masks_dir='data/train/ground_truth',\n",
    "    transform=train_transform,\n",
    "    target_size=(512, 512)\n",
    ")\n",
    "\n",
    "val_dataset = ISICDataset(\n",
    "    images_dir='data/val/images',\n",
    "    masks_dir='data/val/ground_truth',\n",
    "    transform=val_transform,\n",
    "    target_size=(512, 512)\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"üìä Dataset Summary:\")\n",
    "print(f\"   - Training samples: {len(train_dataset)}\")\n",
    "print(f\"   - Validation samples: {len(val_dataset)}\")\n",
    "print(f\"   - Batch size: {batch_size}\")\n",
    "print(f\"   - Training batches: {len(train_loader)}\")\n",
    "print(f\"   - Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SegFormer Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegFormerModel(nn.Module):\n",
    "    def __init__(self, model_name=\"nvidia/segformer-b0-finetuned-ade-512-512\", num_classes=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pre-trained SegFormer\n",
    "        self.segformer = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # Modify the classifier head for binary segmentation\n",
    "        self.segformer.decode_head.classifier = nn.Conv2d(\n",
    "            self.segformer.decode_head.classifier.in_channels,\n",
    "            num_classes,\n",
    "            kernel_size=1\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ SegFormer model loaded: {model_name}\")\n",
    "        print(f\"   - Number of classes: {num_classes}\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # SegFormer forward pass\n",
    "        outputs = self.segformer(pixel_values=x)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Upsample to input resolution\n",
    "        logits = F.interpolate(\n",
    "            logits,\n",
    "            size=x.shape[-2:],  # (H, W)\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        # Apply sigmoid for binary segmentation\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "# Initialize model\n",
    "print(\"ü§ñ Initializing SegFormer model...\")\n",
    "model = SegFormerModel(num_classes=1)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"üìä Model Statistics:\")\n",
    "print(f\"   - Total parameters: {total_params:,}\")\n",
    "print(f\"   - Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   - Model size: ~{total_params/1e6:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Functions v√† Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.view(-1)\n",
    "        target = target.view(-1)\n",
    "        \n",
    "        intersection = (pred * target).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)\n",
    "        \n",
    "        return 1 - dice\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.dice = DiceLoss()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        return self.alpha * self.bce(pred, target) + (1 - self.alpha) * self.dice(pred, target)\n",
    "\n",
    "def calculate_dice_batch(pred, target, threshold=0.5):\n",
    "    \"\"\"Calculate Dice coefficient for a batch\"\"\"\n",
    "    pred_binary = (pred > threshold).float()\n",
    "    target_binary = target.float()\n",
    "    \n",
    "    intersection = (pred_binary * target_binary).sum()\n",
    "    dice = (2. * intersection) / (pred_binary.sum() + target_binary.sum() + 1e-6)\n",
    "    \n",
    "    return dice.item()\n",
    "\n",
    "def calculate_jaccard_batch(pred, target):\n",
    "    \"\"\"Calculate Jaccard Index (IoU) for a batch\"\"\"\n",
    "    intersection = (pred & target).float().sum()\n",
    "    union = (pred | target).float().sum()\n",
    "    \n",
    "    jaccard = intersection / (union + 1e-6)\n",
    "    return jaccard.item()\n",
    "\n",
    "print(\"‚úÖ Loss functions v√† metrics ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_segformer(model, train_loader, val_loader, num_epochs=20, lr=5e-5):\n",
    "    \"\"\"\n",
    "    Training function for SegFormer with Cosine Annealing scheduler\n",
    "    \"\"\"\n",
    "    # Check device and move model to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        print(f\"üöÄ Training on GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"üìä GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Training on CPU - s·∫Ω ch·∫≠m h∆°n r·∫•t nhi·ªÅu!\")\n",
    "        print(\"üí° ƒê·ªÉ s·ª≠ d·ª•ng GPU, c√†i PyTorch v·ªõi CUDA support\")\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = CombinedLoss(alpha=0.5)  # Combination of BCE and Dice loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    \n",
    "    # Cosine Annealing scheduler (ph√π h·ª£p v·ªõi transformer)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=lr*0.01\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_losses': [],\n",
    "        'val_losses': [],\n",
    "        'train_dice': [],\n",
    "        'val_dice': [],\n",
    "        'train_jaccard': [],\n",
    "        'val_jaccard': [],\n",
    "        'learning_rates': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    print(f\"üöÄ B·∫Øt ƒë·∫ßu training SegFormer...\")\n",
    "    print(f\"üìä Configuration:\")\n",
    "    print(f\"   - Epochs: {num_epochs}\")\n",
    "    print(f\"   - Learning Rate: {lr}\")\n",
    "    print(f\"   - Scheduler: Cosine Annealing\")\n",
    "    print(f\"   - Loss: Combined (BCE + Dice)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_dice = 0.0\n",
    "        train_jaccard = 0.0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Training')\n",
    "        for batch_idx, (images, masks) in enumerate(train_pbar):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            with torch.no_grad():\n",
    "                pred_masks = outputs > 0.5\n",
    "                dice = calculate_dice_batch(outputs, masks)\n",
    "                jaccard = calculate_jaccard_batch(pred_masks, masks.bool())\n",
    "                train_dice += dice\n",
    "                train_jaccard += jaccard\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}', \n",
    "                'Dice': f'{dice:.3f}', \n",
    "                'Jaccard': f'{jaccard:.3f}'\n",
    "            })\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_dice = 0.0\n",
    "        val_jaccard = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Validation')\n",
    "            for images, masks in val_pbar:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                pred_masks = outputs > 0.5\n",
    "                dice = calculate_dice_batch(outputs, masks)\n",
    "                jaccard = calculate_jaccard_batch(pred_masks, masks.bool())\n",
    "                val_dice += dice\n",
    "                val_jaccard += jaccard\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_pbar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}', \n",
    "                    'Dice': f'{dice:.3f}', \n",
    "                    'Jaccard': f'{jaccard:.3f}'\n",
    "                })\n",
    "        \n",
    "        # Calculate averages\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        train_dice /= len(train_loader)\n",
    "        val_dice /= len(val_loader)\n",
    "        train_jaccard /= len(train_loader)\n",
    "        val_jaccard /= len(val_loader)\n",
    "        \n",
    "        # Update learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step()\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Store history\n",
    "        history['train_losses'].append(train_loss)\n",
    "        history['val_losses'].append(val_loss)\n",
    "        history['train_dice'].append(train_dice)\n",
    "        history['val_dice'].append(val_dice)\n",
    "        history['train_jaccard'].append(train_jaccard)\n",
    "        history['val_jaccard'].append(val_jaccard)\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        if abs(current_lr - new_lr) > 1e-8:\n",
    "            print(f\"  Learning rate changed: {current_lr:.6f} -> {new_lr:.6f}\")\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Dice: {train_dice:.4f}, Train Jaccard: {train_jaccard:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Dice: {val_dice:.4f}, Val Jaccard: {val_jaccard:.4f}\")\n",
    "        print(f\"  LR: {current_lr:.6f}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'models/segformer_model_best.pth')\n",
    "            print(f\"New best validation loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"‚úÖ Training function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. B·∫Øt ƒë·∫ßu Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SegFormer model\n",
    "print(\"üöÄ B·∫Øt ƒë·∫ßu training SegFormer...\")\n",
    "print(\"‚è∞ Th·ªùi gian d·ª± ki·∫øn: ~2-3 gi·ªù (t√πy thu·ªôc v√†o GPU)\")\n",
    "print()\n",
    "\n",
    "segformer_history = train_segformer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=20,\n",
    "    lr=5e-5\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), 'models/segformer_model_final.pth')\n",
    "print(\"\\nüéâ SegFormer training ho√†n th√†nh!\")\n",
    "print(\"üíæ Model ƒë√£ ƒë∆∞·ª£c l∆∞u:\")\n",
    "print(\"   - models/segformer_model_best.pth (best validation loss)\")\n",
    "print(\"   - models/segformer_model_final.pth (final epoch)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization v√† Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, model_name=\"SegFormer\"):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(history['train_losses'], label='Train Loss', color='blue')\n",
    "    axes[0, 0].plot(history['val_losses'], label='Val Loss', color='red')\n",
    "    axes[0, 0].set_title(f'{model_name} - Training & Validation Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Dice Coefficient\n",
    "    axes[0, 1].plot(history['train_dice'], label='Train Dice', color='blue')\n",
    "    axes[0, 1].plot(history['val_dice'], label='Val Dice', color='red')\n",
    "    axes[0, 1].set_title(f'{model_name} - Dice Coefficient')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Dice Score')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Jaccard Index\n",
    "    axes[1, 0].plot(history['train_jaccard'], label='Train Jaccard', color='blue')\n",
    "    axes[1, 0].plot(history['val_jaccard'], label='Val Jaccard', color='red')\n",
    "    axes[1, 0].set_title(f'{model_name} - Jaccard Index (IoU)')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Jaccard Score')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # Learning Rate\n",
    "    axes[1, 1].plot(history['learning_rates'], label='Learning Rate', color='green')\n",
    "    axes[1, 1].set_title(f'{model_name} - Learning Rate Schedule')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Learning Rate')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "print(\"üìä Hi·ªÉn th·ªã training history:\")\n",
    "plot_training_history(segformer_history, \"SegFormer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_samples(model, val_loader, num_samples=6):\n",
    "    \"\"\"Evaluate model on sample images\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get some samples\n",
    "    samples = []\n",
    "    with torch.no_grad():\n",
    "        for i, (images, masks) in enumerate(val_loader):\n",
    "            if i >= num_samples // val_loader.batch_size + 1:\n",
    "                break\n",
    "                \n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            for j in range(min(images.shape[0], num_samples - len(samples))):\n",
    "                # Denormalize image for visualization\n",
    "                img = images[j].cpu()\n",
    "                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "                img = img * std + mean\n",
    "                img = torch.clamp(img, 0, 1)\n",
    "                \n",
    "                samples.append({\n",
    "                    'image': img.permute(1, 2, 0).numpy(),\n",
    "                    'true_mask': masks[j, 0].cpu().numpy(),\n",
    "                    'pred_mask': outputs[j, 0].cpu().numpy(),\n",
    "                    'pred_binary': (outputs[j, 0] > 0.5).cpu().numpy().astype(np.uint8)\n",
    "                })\n",
    "                \n",
    "                if len(samples) >= num_samples:\n",
    "                    break\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, axes = plt.subplots(4, len(samples), figsize=(20, 16))\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        # Original image\n",
    "        axes[0, i].imshow(sample['image'])\n",
    "        axes[0, i].set_title(f'Original {i+1}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # True mask\n",
    "        axes[1, i].imshow(sample['true_mask'], cmap='gray')\n",
    "        axes[1, i].set_title(f'True Mask {i+1}')\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # Predicted mask (probability)\n",
    "        axes[2, i].imshow(sample['pred_mask'], cmap='hot', vmin=0, vmax=1)\n",
    "        axes[2, i].set_title(f'Pred Prob {i+1}')\n",
    "        axes[2, i].axis('off')\n",
    "        \n",
    "        # Predicted mask (binary)\n",
    "        axes[3, i].imshow(sample['pred_binary'], cmap='gray')\n",
    "        axes[3, i].set_title(f'Pred Binary {i+1}')\n",
    "        axes[3, i].axis('off')\n",
    "    \n",
    "    plt.suptitle('SegFormer - Prediction Results', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate model on samples\n",
    "print(\"üîç ƒê√°nh gi√° model tr√™n validation samples:\")\n",
    "evaluate_model_on_samples(model, val_loader, num_samples=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final results\n",
    "print(\"üéØ SEGFORMER TRAINING RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìä Final Metrics (Last Epoch):\")\n",
    "print(f\"   - Training Loss: {segformer_history['train_losses'][-1]:.4f}\")\n",
    "print(f\"   - Validation Loss: {segformer_history['val_losses'][-1]:.4f}\")\n",
    "print(f\"   - Training Dice: {segformer_history['train_dice'][-1]:.4f}\")\n",
    "print(f\"   - Validation Dice: {segformer_history['val_dice'][-1]:.4f}\")\n",
    "print(f\"   - Training Jaccard: {segformer_history['train_jaccard'][-1]:.4f}\")\n",
    "print(f\"   - Validation Jaccard: {segformer_history['val_jaccard'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Metrics:\")\n",
    "best_val_dice_idx = np.argmax(segformer_history['val_dice'])\n",
    "best_val_jaccard_idx = np.argmax(segformer_history['val_jaccard'])\n",
    "print(f\"   - Best Validation Dice: {max(segformer_history['val_dice']):.4f} (Epoch {best_val_dice_idx + 1})\")\n",
    "print(f\"   - Best Validation Jaccard: {max(segformer_history['val_jaccard']):.4f} (Epoch {best_val_jaccard_idx + 1})\")\n",
    "\n",
    "print(f\"\\nüíæ Saved Models:\")\n",
    "print(f\"   - models/segformer_model_best.pth\")\n",
    "print(f\"   - models/segformer_model_final.pth\")\n",
    "\n",
    "print(f\"\\nüìà Model Performance:\")\n",
    "final_dice = segformer_history['val_dice'][-1]\n",
    "final_jaccard = segformer_history['val_jaccard'][-1]\n",
    "\n",
    "if final_dice > 0.85:\n",
    "    print(f\"   ‚úÖ Excellent performance! Dice > 0.85\")\n",
    "elif final_dice > 0.80:\n",
    "    print(f\"   ‚úÖ Good performance! Dice > 0.80\")\n",
    "elif final_dice > 0.75:\n",
    "    print(f\"   ‚ö†Ô∏è  Acceptable performance. Dice > 0.75\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Performance needs improvement. Dice < 0.75\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(f\"   1. Run 03_train_unet_efficientnet.ipynb to train U-Net EfficientNet\")\n",
    "print(f\"   2. Run 04_train_unet_vit.ipynb to train U-Net ViT\")\n",
    "print(f\"   3. Run 05_train_deeplabv3_resnet.ipynb to train DeepLabV3+ ResNet\")\n",
    "print(f\"   4. Compare all models' performance\")\n",
    "\n",
    "print(\"\\n‚úÖ SegFormer training completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
