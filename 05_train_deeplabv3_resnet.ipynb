{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎯 Training DeepLabV3+ với ResNet Backbone\n",
    "\n",
    "Notebook này thực hiện training model **DeepLabV3+ với ResNet backbone** cho bài toán phân đoạn khối u da:\n",
    "\n",
    "## 🎯 Model: DeepLabV3+ + ResNet\n",
    "- **Architecture**: DeepLabV3+ với ResNet-50 encoder\n",
    "- **Backbone**: ResNet-50 (pre-trained trên ImageNet)\n",
    "- **Parameters**: ~39.6M\n",
    "- **Ưu điểm**: Robust, xử lý tốt multi-scale objects, ASPP module cho multi-scale features\n",
    "- **Learning Rate Scheduler**: StepLR (scheduled decay)\n",
    "\n",
    "## 📊 Training Configuration:\n",
    "- **Epochs**: 15\n",
    "- **Learning Rate**: 1e-4\n",
    "- **Batch Size**: 8\n",
    "- **Optimizer**: Adam với weight decay 1e-4\n",
    "- **Loss Function**: Combined Loss (BCE + Dice)\n",
    "- **Metrics**: Dice Coefficient, Jaccard Index (IoU)\n",
    "- **Scheduler**: StepLR (step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import thư viện và setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cài đặt các thư viện cần thiết\n",
    "!pip install segmentation-models-pytorch timm torch torchvision\n",
    "!pip install albumentations opencv-python-headless\n",
    "!pip install matplotlib seaborn scikit-learn pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Segmentation models\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# Timm for backbone\n",
    "import timm\n",
    "\n",
    "# Albumentations for augmentation\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import jaccard_score, f1_score\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Tạo thư mục models nếu chưa có\n",
    "os.makedirs('models', exist_ok=True)\n",
    "print(\"✅ Setup hoàn tất!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset và Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISICDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None, target_size=(512, 512)):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        # Lấy danh sách file images\n",
    "        self.image_files = sorted([f for f in os.listdir(images_dir) \n",
    "                                 if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        \n",
    "        print(f\"Found {len(self.image_files)} images in {images_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Find corresponding mask\n",
    "        base_name = os.path.splitext(img_name)[0]\n",
    "        possible_mask_names = [\n",
    "            f\"{base_name}_segmentation.png\",\n",
    "            f\"{base_name}_mask.png\",\n",
    "            f\"{base_name}.png\"\n",
    "        ]\n",
    "        \n",
    "        mask = None\n",
    "        for mask_name in possible_mask_names:\n",
    "            mask_path = os.path.join(self.masks_dir, mask_name)\n",
    "            if os.path.exists(mask_path):\n",
    "                mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "                break\n",
    "        \n",
    "        if mask is None:\n",
    "            # Create dummy mask if not found\n",
    "            mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "        \n",
    "        # Resize\n",
    "        image = cv2.resize(image, self.target_size)\n",
    "        mask = cv2.resize(mask, self.target_size)\n",
    "        \n",
    "        # Convert mask to binary\n",
    "        mask = (mask > 127).astype(np.float32)\n",
    "        \n",
    "        # Apply augmentations\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        \n",
    "        # Convert to tensor if not already\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n",
    "        \n",
    "        if not isinstance(mask, torch.Tensor):\n",
    "            mask = torch.from_numpy(mask).float().unsqueeze(0)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# Define augmentations\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "print(\"✅ Dataset class và augmentations đã được định nghĩa!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = ISICDataset(\n",
    "    images_dir='data/train/images',\n",
    "    masks_dir='data/train/ground_truth',\n",
    "    transform=train_transform,\n",
    "    target_size=(512, 512)\n",
    ")\n",
    "\n",
    "val_dataset = ISICDataset(\n",
    "    images_dir='data/val/images',\n",
    "    masks_dir='data/val/ground_truth',\n",
    "    transform=val_transform,\n",
    "    target_size=(512, 512)\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"📊 Dataset Summary:\")\n",
    "print(f\"   - Training samples: {len(train_dataset)}\")\n",
    "print(f\"   - Validation samples: {len(val_dataset)}\")\n",
    "print(f\"   - Batch size: {batch_size}\")\n",
    "print(f\"   - Training batches: {len(train_loader)}\")\n",
    "print(f\"   - Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DeepLabV3+ ResNet Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabV3PlusResNet(nn.Module):\n",
    "    def __init__(self, backbone_name=\"resnet50\", num_classes=1, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Sử dụng segmentation_models_pytorch để tạo DeepLabV3+ với ResNet backbone\n",
    "        self.model = smp.DeepLabV3Plus(\n",
    "            encoder_name=backbone_name,\n",
    "            encoder_weights=\"imagenet\" if pretrained else None,\n",
    "            in_channels=3,\n",
    "            classes=num_classes,\n",
    "            activation=None  # We'll apply sigmoid manually\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ DeepLabV3+ ResNet model created:\")\n",
    "        print(f\"   - Backbone: {backbone_name}\")\n",
    "        print(f\"   - Pre-trained: {pretrained}\")\n",
    "        print(f\"   - Number of classes: {num_classes}\")\n",
    "        print(f\"   - Features: ASPP module, multi-scale processing\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through DeepLabV3+\n",
    "        logits = self.model(x)\n",
    "        \n",
    "        # Apply sigmoid for binary segmentation\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "# Initialize model\n",
    "print(\"🎯 Initializing DeepLabV3+ ResNet model...\")\n",
    "model = DeepLabV3PlusResNet(\n",
    "    backbone_name=\"resnet50\",\n",
    "    num_classes=1,\n",
    "    pretrained=True\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"📊 Model Statistics:\")\n",
    "print(f\"   - Total parameters: {total_params:,}\")\n",
    "print(f\"   - Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   - Model size: ~{total_params/1e6:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Functions và Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.view(-1)\n",
    "        target = target.view(-1)\n",
    "        \n",
    "        intersection = (pred * target).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)\n",
    "        \n",
    "        return 1 - dice\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.dice = DiceLoss()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        return self.alpha * self.bce(pred, target) + (1 - self.alpha) * self.dice(pred, target)\n",
    "\n",
    "def calculate_dice_batch(pred, target, threshold=0.5):\n",
    "    \"\"\"Calculate Dice coefficient for a batch\"\"\"\n",
    "    pred_binary = (pred > threshold).float()\n",
    "    target_binary = target.float()\n",
    "    \n",
    "    intersection = (pred_binary * target_binary).sum()\n",
    "    dice = (2. * intersection) / (pred_binary.sum() + target_binary.sum() + 1e-6)\n",
    "    \n",
    "    return dice.item()\n",
    "\n",
    "def calculate_jaccard_batch(pred, target):\n",
    "    \"\"\"Calculate Jaccard Index (IoU) for a batch\"\"\"\n",
    "    intersection = (pred & target).float().sum()\n",
    "    union = (pred | target).float().sum()\n",
    "    \n",
    "    jaccard = intersection / (union + 1e-6)\n",
    "    return jaccard.item()\n",
    "\n",
    "print(\"✅ Loss functions và metrics đã được định nghĩa!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Function với StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deeplabv3_resnet(model, train_loader, val_loader, num_epochs=15, lr=1e-4):\n",
    "    \"\"\"\n",
    "    Training function for DeepLabV3+ ResNet with StepLR scheduler\n",
    "    \"\"\"\n",
    "    # Loss function and optimizer\n",
    "    criterion = CombinedLoss(alpha=0.5)  # Combination of BCE and Dice loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    \n",
    "    # StepLR scheduler (scheduled decay)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=5, gamma=0.1\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_losses': [],\n",
    "        'val_losses': [],\n",
    "        'train_dice': [],\n",
    "        'val_dice': [],\n",
    "        'train_jaccard': [],\n",
    "        'val_jaccard': [],\n",
    "        'learning_rates': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    print(f\"🚀 Bắt đầu training DeepLabV3+ ResNet...\")\n",
    "    print(f\"📊 Configuration:\")\n",
    "    print(f\"   - Epochs: {num_epochs}\")\n",
    "    print(f\"   - Learning Rate: {lr}\")\n",
    "    print(f\"   - Scheduler: StepLR (step_size=5, gamma=0.1)\")\n",
    "    print(f\"   - Loss: Combined (BCE + Dice)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_dice = 0.0\n",
    "        train_jaccard = 0.0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Training')\n",
    "        for batch_idx, (images, masks) in enumerate(train_pbar):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            with torch.no_grad():\n",
    "                pred_masks = outputs > 0.5\n",
    "                dice = calculate_dice_batch(outputs, masks)\n",
    "                jaccard = calculate_jaccard_batch(pred_masks, masks.bool())\n",
    "                train_dice += dice\n",
    "                train_jaccard += jaccard\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}', \n",
    "                'Dice': f'{dice:.3f}', \n",
    "                'Jaccard': f'{jaccard:.3f}'\n",
    "            })\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_dice = 0.0\n",
    "        val_jaccard = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Validation')\n",
    "            for images, masks in val_pbar:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                pred_masks = outputs > 0.5\n",
    "                dice = calculate_dice_batch(outputs, masks)\n",
    "                jaccard = calculate_jaccard_batch(pred_masks, masks.bool())\n",
    "                val_dice += dice\n",
    "                val_jaccard += jaccard\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_pbar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}', \n",
    "                    'Dice': f'{dice:.3f}', \n",
    "                    'Jaccard': f'{jaccard:.3f}'\n",
    "                })\n",
    "        \n",
    "        # Calculate averages\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        train_dice /= len(train_loader)\n",
    "        val_dice /= len(val_loader)\n",
    "        train_jaccard /= len(train_loader)\n",
    "        val_jaccard /= len(val_loader)\n",
    "        \n",
    "        # Update learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step()\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Store history\n",
    "        history['train_losses'].append(train_loss)\n",
    "        history['val_losses'].append(val_loss)\n",
    "        history['train_dice'].append(train_dice)\n",
    "        history['val_dice'].append(val_dice)\n",
    "        history['train_jaccard'].append(train_jaccard)\n",
    "        history['val_jaccard'].append(val_jaccard)\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        if abs(current_lr - new_lr) > 1e-8:\n",
    "            print(f\"  Learning rate changed: {current_lr:.6f} -> {new_lr:.6f}\")\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Dice: {train_dice:.4f}, Train Jaccard: {train_jaccard:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Dice: {val_dice:.4f}, Val Jaccard: {val_jaccard:.4f}\")\n",
    "        print(f\"  LR: {current_lr:.6f}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'models/deeplabv3_resnet_model_best.pth')\n",
    "            print(f\"New best validation loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"✅ Training function đã được định nghĩa!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bắt đầu Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the DeepLabV3+ ResNet model\n",
    "print(\"🚀 Bắt đầu training DeepLabV3+ ResNet...\")\n",
    "print(\"⏰ Thời gian dự kiến: ~3-4 giờ (tùy thuộc vào GPU)\")\n",
    "print()\n",
    "\n",
    "deeplabv3_resnet_history = train_deeplabv3_resnet(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=15,\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), 'models/deeplabv3_resnet_model_final.pth')\n",
    "print(\"\\n🎉 DeepLabV3+ ResNet training hoàn thành!\")\n",
    "print(\"💾 Model đã được lưu:\")\n",
    "print(\"   - models/deeplabv3_resnet_model_best.pth (best validation loss)\")\n",
    "print(\"   - models/deeplabv3_resnet_model_final.pth (final epoch)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization và Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, model_name=\"DeepLabV3+ ResNet\"):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(history['train_losses'], label='Train Loss', color='blue')\n",
    "    axes[0, 0].plot(history['val_losses'], label='Val Loss', color='red')\n",
    "    axes[0, 0].set_title(f'{model_name} - Training & Validation Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Dice Coefficient\n",
    "    axes[0, 1].plot(history['train_dice'], label='Train Dice', color='blue')\n",
    "    axes[0, 1].plot(history['val_dice'], label='Val Dice', color='red')\n",
    "    axes[0, 1].set_title(f'{model_name} - Dice Coefficient')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Dice Score')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Jaccard Index\n",
    "    axes[1, 0].plot(history['train_jaccard'], label='Train Jaccard', color='blue')\n",
    "    axes[1, 0].plot(history['val_jaccard'], label='Val Jaccard', color='red')\n",
    "    axes[1, 0].set_title(f'{model_name} - Jaccard Index (IoU)')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Jaccard Score')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # Learning Rate\n",
    "    axes[1, 1].plot(history['learning_rates'], label='Learning Rate', color='green')\n",
    "    axes[1, 1].set_title(f'{model_name} - Learning Rate Schedule (StepLR)')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Learning Rate')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "print(\"📊 Hiển thị training history:\")\n",
    "plot_training_history(deeplabv3_resnet_history, \"DeepLabV3+ ResNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_samples(model, val_loader, num_samples=6):\n",
    "    \"\"\"Evaluate model on sample images\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get some samples\n",
    "    samples = []\n",
    "    with torch.no_grad():\n",
    "        for i, (images, masks) in enumerate(val_loader):\n",
    "            if i >= num_samples // val_loader.batch_size + 1:\n",
    "                break\n",
    "                \n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            for j in range(min(images.shape[0], num_samples - len(samples))):\n",
    "                # Denormalize image for visualization\n",
    "                img = images[j].cpu()\n",
    "                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "                img = img * std + mean\n",
    "                img = torch.clamp(img, 0, 1)\n",
    "                \n",
    "                samples.append({\n",
    "                    'image': img.permute(1, 2, 0).numpy(),\n",
    "                    'true_mask': masks[j, 0].cpu().numpy(),\n",
    "                    'pred_mask': outputs[j, 0].cpu().numpy(),\n",
    "                    'pred_binary': (outputs[j, 0] > 0.5).cpu().numpy().astype(np.uint8)\n",
    "                })\n",
    "                \n",
    "                if len(samples) >= num_samples:\n",
    "                    break\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, axes = plt.subplots(4, len(samples), figsize=(20, 16))\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        # Original image\n",
    "        axes[0, i].imshow(sample['image'])\n",
    "        axes[0, i].set_title(f'Original {i+1}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # True mask\n",
    "        axes[1, i].imshow(sample['true_mask'], cmap='gray')\n",
    "        axes[1, i].set_title(f'True Mask {i+1}')\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # Predicted mask (probability)\n",
    "        axes[2, i].imshow(sample['pred_mask'], cmap='hot', vmin=0, vmax=1)\n",
    "        axes[2, i].set_title(f'Pred Prob {i+1}')\n",
    "        axes[2, i].axis('off')\n",
    "        \n",
    "        # Predicted mask (binary)\n",
    "        axes[3, i].imshow(sample['pred_binary'], cmap='gray')\n",
    "        axes[3, i].set_title(f'Pred Binary {i+1}')\n",
    "        axes[3, i].axis('off')\n",
    "    \n",
    "    plt.suptitle('DeepLabV3+ ResNet - Prediction Results', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate model on samples\n",
    "print(\"🔍 Đánh giá model trên validation samples:\")\n",
    "evaluate_model_on_samples(model, val_loader, num_samples=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final results\n",
    "print(\"🎯 DEEPLABV3+ RESNET TRAINING RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"📊 Final Metrics (Last Epoch):\")\n",
    "print(f\"   - Training Loss: {deeplabv3_resnet_history['train_losses'][-1]:.4f}\")\n",
    "print(f\"   - Validation Loss: {deeplabv3_resnet_history['val_losses'][-1]:.4f}\")\n",
    "print(f\"   - Training Dice: {deeplabv3_resnet_history['train_dice'][-1]:.4f}\")\n",
    "print(f\"   - Validation Dice: {deeplabv3_resnet_history['val_dice'][-1]:.4f}\")\n",
    "print(f\"   - Training Jaccard: {deeplabv3_resnet_history['train_jaccard'][-1]:.4f}\")\n",
    "print(f\"   - Validation Jaccard: {deeplabv3_resnet_history['val_jaccard'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\n🏆 Best Metrics:\")\n",
    "best_val_dice_idx = np.argmax(deeplabv3_resnet_history['val_dice'])\n",
    "best_val_jaccard_idx = np.argmax(deeplabv3_resnet_history['val_jaccard'])\n",
    "print(f\"   - Best Validation Dice: {max(deeplabv3_resnet_history['val_dice']):.4f} (Epoch {best_val_dice_idx + 1})\")\n",
    "print(f\"   - Best Validation Jaccard: {max(deeplabv3_resnet_history['val_jaccard']):.4f} (Epoch {best_val_jaccard_idx + 1})\")\n",
    "\n",
    "print(f\"\\n💾 Saved Models:\")\n",
    "print(f\"   - models/deeplabv3_resnet_model_best.pth\")\n",
    "print(f\"   - models/deeplabv3_resnet_model_final.pth\")\n",
    "\n",
    "print(f\"\\n📈 Model Performance:\")\n",
    "final_dice = deeplabv3_resnet_history['val_dice'][-1]\n",
    "final_jaccard = deeplabv3_resnet_history['val_jaccard'][-1]\n",
    "\n",
    "if final_dice > 0.85:\n",
    "    print(f\"   ✅ Excellent performance! Dice > 0.85\")\n",
    "elif final_dice > 0.80:\n",
    "    print(f\"   ✅ Good performance! Dice > 0.80\")\n",
    "elif final_dice > 0.75:\n",
    "    print(f\"   ⚠️  Acceptable performance. Dice > 0.75\")\n",
    "else:\n",
    "    print(f\"   ❌ Performance needs improvement. Dice < 0.75\")\n",
    "\n",
    "print(f\"\\n🎉 ALL MODELS TRAINING COMPLETED!\")\n",
    "print(f\"\\n📊 Trained Models Summary:\")\n",
    "print(f\"   1. ✅ SegFormer (Transformer-based) - models/segformer_model_*.pth\")\n",
    "print(f\"   2. ✅ U-Net EfficientNet (CNN-based) - models/unet_efficientnet_model_*.pth\")\n",
    "print(f\"   3. ✅ U-Net ViT (Hybrid) - models/unet_vit_model_*.pth\")\n",
    "print(f\"   4. ✅ DeepLabV3+ ResNet (Multi-scale) - models/deeplabv3_resnet_model_*.pth\")\n",
    "\n",
    "print(f\"\\n🚀 Next Steps:\")\n",
    "print(f\"   1. Compare all models' performance\")\n",
    "print(f\"   2. Create ensemble predictions\")\n",
    "print(f\"   3. Evaluate on test set\")\n",
    "print(f\"   4. Deploy best performing model\")\n",
    "\n",
    "print(\"\\n✅ DeepLabV3+ ResNet training completed successfully!\")\n",
    "print(\"🎊 Congratulations! All 4 models have been trained successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
