{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Training DeepLabV3+ v·ªõi ResNet Backbone\n",
    "\n",
    "Notebook n√†y th·ª±c hi·ªán training model **DeepLabV3+ v·ªõi ResNet backbone** cho b√†i to√°n ph√¢n ƒëo·∫°n kh·ªëi u da:\n",
    "\n",
    "## üéØ Model: DeepLabV3+ + ResNet\n",
    "- **Architecture**: DeepLabV3+ v·ªõi ResNet-50 encoder\n",
    "- **Backbone**: ResNet-50 (pre-trained tr√™n ImageNet)\n",
    "- **Parameters**: ~39.6M\n",
    "- **∆Øu ƒëi·ªÉm**: Robust, x·ª≠ l√Ω t·ªët multi-scale objects, ASPP module cho multi-scale features\n",
    "- **Learning Rate Scheduler**: StepLR (scheduled decay)\n",
    "\n",
    "## üìä Training Configuration:\n",
    "- **Epochs**: 15\n",
    "- **Learning Rate**: 1e-4\n",
    "- **Batch Size**: 8\n",
    "- **Optimizer**: Adam v·ªõi weight decay 1e-4\n",
    "- **Loss Function**: Combined Loss (BCE + Dice)\n",
    "- **Metrics**: Dice Coefficient, Jaccard Index (IoU)\n",
    "- **Scheduler**: StepLR (step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import th∆∞ vi·ªán v√† setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "!pip install segmentation-models-pytorch timm torch torchvision\n",
    "!pip install albumentations opencv-python-headless\n",
    "!pip install matplotlib seaborn scikit-learn pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Segmentation models\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# Timm for backbone\n",
    "import timm\n",
    "\n",
    "# Albumentations for augmentation\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import jaccard_score, f1_score\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c models n·∫øu ch∆∞a c√≥\n",
    "os.makedirs('models', exist_ok=True)\n",
    "print(\"‚úÖ Setup ho√†n t·∫•t!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset v√† Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISICDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None, target_size=(512, 512)):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        # L·∫•y danh s√°ch file images\n",
    "        self.image_files = sorted([f for f in os.listdir(images_dir) \n",
    "                                 if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        \n",
    "        print(f\"Found {len(self.image_files)} images in {images_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Find corresponding mask\n",
    "        base_name = os.path.splitext(img_name)[0]\n",
    "        possible_mask_names = [\n",
    "            f\"{base_name}_segmentation.png\",\n",
    "            f\"{base_name}_mask.png\",\n",
    "            f\"{base_name}.png\"\n",
    "        ]\n",
    "        \n",
    "        mask = None\n",
    "        for mask_name in possible_mask_names:\n",
    "            mask_path = os.path.join(self.masks_dir, mask_name)\n",
    "            if os.path.exists(mask_path):\n",
    "                mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "                break\n",
    "        \n",
    "        if mask is None:\n",
    "            # Create dummy mask if not found\n",
    "            mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "        \n",
    "        # Resize\n",
    "        image = cv2.resize(image, self.target_size)\n",
    "        mask = cv2.resize(mask, self.target_size)\n",
    "        \n",
    "        # Convert mask to binary\n",
    "        mask = (mask > 127).astype(np.float32)\n",
    "        \n",
    "        # Apply augmentations\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        \n",
    "        # Convert to tensor if not already\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n",
    "        \n",
    "        if not isinstance(mask, torch.Tensor):\n",
    "            mask = torch.from_numpy(mask).float().unsqueeze(0)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# Define augmentations\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Dataset class v√† augmentations ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = ISICDataset(\n",
    "    images_dir='data/train/images',\n",
    "    masks_dir='data/train/ground_truth',\n",
    "    transform=train_transform,\n",
    "    target_size=(512, 512)\n",
    ")\n",
    "\n",
    "val_dataset = ISICDataset(\n",
    "    images_dir='data/val/images',\n",
    "    masks_dir='data/val/ground_truth',\n",
    "    transform=val_transform,\n",
    "    target_size=(512, 512)\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"üìä Dataset Summary:\")\n",
    "print(f\"   - Training samples: {len(train_dataset)}\")\n",
    "print(f\"   - Validation samples: {len(val_dataset)}\")\n",
    "print(f\"   - Batch size: {batch_size}\")\n",
    "print(f\"   - Training batches: {len(train_loader)}\")\n",
    "print(f\"   - Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DeepLabV3+ ResNet Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabV3PlusResNet(nn.Module):\n",
    "    def __init__(self, backbone_name=\"resnet50\", num_classes=1, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # S·ª≠ d·ª•ng segmentation_models_pytorch ƒë·ªÉ t·∫°o DeepLabV3+ v·ªõi ResNet backbone\n",
    "        self.model = smp.DeepLabV3Plus(\n",
    "            encoder_name=backbone_name,\n",
    "            encoder_weights=\"imagenet\" if pretrained else None,\n",
    "            in_channels=3,\n",
    "            classes=num_classes,\n",
    "            activation=None  # We'll apply sigmoid manually\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ DeepLabV3+ ResNet model created:\")\n",
    "        print(f\"   - Backbone: {backbone_name}\")\n",
    "        print(f\"   - Pre-trained: {pretrained}\")\n",
    "        print(f\"   - Number of classes: {num_classes}\")\n",
    "        print(f\"   - Features: ASPP module, multi-scale processing\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through DeepLabV3+\n",
    "        logits = self.model(x)\n",
    "        \n",
    "        # Apply sigmoid for binary segmentation\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "# Initialize model\n",
    "print(\"üéØ Initializing DeepLabV3+ ResNet model...\")\n",
    "model = DeepLabV3PlusResNet(\n",
    "    backbone_name=\"resnet50\",\n",
    "    num_classes=1,\n",
    "    pretrained=True\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"üìä Model Statistics:\")\n",
    "print(f\"   - Total parameters: {total_params:,}\")\n",
    "print(f\"   - Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   - Model size: ~{total_params/1e6:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Functions v√† Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.view(-1)\n",
    "        target = target.view(-1)\n",
    "        \n",
    "        intersection = (pred * target).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)\n",
    "        \n",
    "        return 1 - dice\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.dice = DiceLoss()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        return self.alpha * self.bce(pred, target) + (1 - self.alpha) * self.dice(pred, target)\n",
    "\n",
    "def calculate_dice_batch(pred, target, threshold=0.5):\n",
    "    \"\"\"Calculate Dice coefficient for a batch\"\"\"\n",
    "    pred_binary = (pred > threshold).float()\n",
    "    target_binary = target.float()\n",
    "    \n",
    "    intersection = (pred_binary * target_binary).sum()\n",
    "    dice = (2. * intersection) / (pred_binary.sum() + target_binary.sum() + 1e-6)\n",
    "    \n",
    "    return dice.item()\n",
    "\n",
    "def calculate_jaccard_batch(pred, target):\n",
    "    \"\"\"Calculate Jaccard Index (IoU) for a batch\"\"\"\n",
    "    intersection = (pred & target).float().sum()\n",
    "    union = (pred | target).float().sum()\n",
    "    \n",
    "    jaccard = intersection / (union + 1e-6)\n",
    "    return jaccard.item()\n",
    "\n",
    "print(\"‚úÖ Loss functions v√† metrics ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Function v·ªõi StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deeplabv3_resnet(model, train_loader, val_loader, num_epochs=15, lr=1e-4):\n",
    "    \"\"\"\n",
    "    Training function for DeepLabV3+ ResNet with StepLR scheduler\n",
    "    \"\"\"\n",
    "    # Loss function and optimizer\n",
    "    criterion = CombinedLoss(alpha=0.5)  # Combination of BCE and Dice loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    \n",
    "    # StepLR scheduler (scheduled decay)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=5, gamma=0.1\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_losses': [],\n",
    "        'val_losses': [],\n",
    "        'train_dice': [],\n",
    "        'val_dice': [],\n",
    "        'train_jaccard': [],\n",
    "        'val_jaccard': [],\n",
    "        'learning_rates': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    print(f\"üöÄ B·∫Øt ƒë·∫ßu training DeepLabV3+ ResNet...\")\n",
    "    print(f\"üìä Configuration:\")\n",
    "    print(f\"   - Epochs: {num_epochs}\")\n",
    "    print(f\"   - Learning Rate: {lr}\")\n",
    "    print(f\"   - Scheduler: StepLR (step_size=5, gamma=0.1)\")\n",
    "    print(f\"   - Loss: Combined (BCE + Dice)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_dice = 0.0\n",
    "        train_jaccard = 0.0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Training')\n",
    "        for batch_idx, (images, masks) in enumerate(train_pbar):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            with torch.no_grad():\n",
    "                pred_masks = outputs > 0.5\n",
    "                dice = calculate_dice_batch(outputs, masks)\n",
    "                jaccard = calculate_jaccard_batch(pred_masks, masks.bool())\n",
    "                train_dice += dice\n",
    "                train_jaccard += jaccard\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}', \n",
    "                'Dice': f'{dice:.3f}', \n",
    "                'Jaccard': f'{jaccard:.3f}'\n",
    "            })\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_dice = 0.0\n",
    "        val_jaccard = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Validation')\n",
    "            for images, masks in val_pbar:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                pred_masks = outputs > 0.5\n",
    "                dice = calculate_dice_batch(outputs, masks)\n",
    "                jaccard = calculate_jaccard_batch(pred_masks, masks.bool())\n",
    "                val_dice += dice\n",
    "                val_jaccard += jaccard\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_pbar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}', \n",
    "                    'Dice': f'{dice:.3f}', \n",
    "                    'Jaccard': f'{jaccard:.3f}'\n",
    "                })\n",
    "        \n",
    "        # Calculate averages\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        train_dice /= len(train_loader)\n",
    "        val_dice /= len(val_loader)\n",
    "        train_jaccard /= len(train_loader)\n",
    "        val_jaccard /= len(val_loader)\n",
    "        \n",
    "        # Update learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step()\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Store history\n",
    "        history['train_losses'].append(train_loss)\n",
    "        history['val_losses'].append(val_loss)\n",
    "        history['train_dice'].append(train_dice)\n",
    "        history['val_dice'].append(val_dice)\n",
    "        history['train_jaccard'].append(train_jaccard)\n",
    "        history['val_jaccard'].append(val_jaccard)\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        if abs(current_lr - new_lr) > 1e-8:\n",
    "            print(f\"  Learning rate changed: {current_lr:.6f} -> {new_lr:.6f}\")\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Dice: {train_dice:.4f}, Train Jaccard: {train_jaccard:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Dice: {val_dice:.4f}, Val Jaccard: {val_jaccard:.4f}\")\n",
    "        print(f\"  LR: {current_lr:.6f}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'models/deeplabv3_resnet_model_best.pth')\n",
    "            print(f\"New best validation loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"‚úÖ Training function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. B·∫Øt ƒë·∫ßu Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the DeepLabV3+ ResNet model\n",
    "print(\"üöÄ B·∫Øt ƒë·∫ßu training DeepLabV3+ ResNet...\")\n",
    "print(\"‚è∞ Th·ªùi gian d·ª± ki·∫øn: ~3-4 gi·ªù (t√πy thu·ªôc v√†o GPU)\")\n",
    "print()\n",
    "\n",
    "deeplabv3_resnet_history = train_deeplabv3_resnet(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=15,\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), 'models/deeplabv3_resnet_model_final.pth')\n",
    "print(\"\\nüéâ DeepLabV3+ ResNet training ho√†n th√†nh!\")\n",
    "print(\"üíæ Model ƒë√£ ƒë∆∞·ª£c l∆∞u:\")\n",
    "print(\"   - models/deeplabv3_resnet_model_best.pth (best validation loss)\")\n",
    "print(\"   - models/deeplabv3_resnet_model_final.pth (final epoch)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization v√† Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, model_name=\"DeepLabV3+ ResNet\"):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(history['train_losses'], label='Train Loss', color='blue')\n",
    "    axes[0, 0].plot(history['val_losses'], label='Val Loss', color='red')\n",
    "    axes[0, 0].set_title(f'{model_name} - Training & Validation Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Dice Coefficient\n",
    "    axes[0, 1].plot(history['train_dice'], label='Train Dice', color='blue')\n",
    "    axes[0, 1].plot(history['val_dice'], label='Val Dice', color='red')\n",
    "    axes[0, 1].set_title(f'{model_name} - Dice Coefficient')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Dice Score')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Jaccard Index\n",
    "    axes[1, 0].plot(history['train_jaccard'], label='Train Jaccard', color='blue')\n",
    "    axes[1, 0].plot(history['val_jaccard'], label='Val Jaccard', color='red')\n",
    "    axes[1, 0].set_title(f'{model_name} - Jaccard Index (IoU)')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Jaccard Score')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # Learning Rate\n",
    "    axes[1, 1].plot(history['learning_rates'], label='Learning Rate', color='green')\n",
    "    axes[1, 1].set_title(f'{model_name} - Learning Rate Schedule (StepLR)')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Learning Rate')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "print(\"üìä Hi·ªÉn th·ªã training history:\")\n",
    "plot_training_history(deeplabv3_resnet_history, \"DeepLabV3+ ResNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_samples(model, val_loader, num_samples=6):\n",
    "    \"\"\"Evaluate model on sample images\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get some samples\n",
    "    samples = []\n",
    "    with torch.no_grad():\n",
    "        for i, (images, masks) in enumerate(val_loader):\n",
    "            if i >= num_samples // val_loader.batch_size + 1:\n",
    "                break\n",
    "                \n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            for j in range(min(images.shape[0], num_samples - len(samples))):\n",
    "                # Denormalize image for visualization\n",
    "                img = images[j].cpu()\n",
    "                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "                img = img * std + mean\n",
    "                img = torch.clamp(img, 0, 1)\n",
    "                \n",
    "                samples.append({\n",
    "                    'image': img.permute(1, 2, 0).numpy(),\n",
    "                    'true_mask': masks[j, 0].cpu().numpy(),\n",
    "                    'pred_mask': outputs[j, 0].cpu().numpy(),\n",
    "                    'pred_binary': (outputs[j, 0] > 0.5).cpu().numpy().astype(np.uint8)\n",
    "                })\n",
    "                \n",
    "                if len(samples) >= num_samples:\n",
    "                    break\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, axes = plt.subplots(4, len(samples), figsize=(20, 16))\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        # Original image\n",
    "        axes[0, i].imshow(sample['image'])\n",
    "        axes[0, i].set_title(f'Original {i+1}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # True mask\n",
    "        axes[1, i].imshow(sample['true_mask'], cmap='gray')\n",
    "        axes[1, i].set_title(f'True Mask {i+1}')\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # Predicted mask (probability)\n",
    "        axes[2, i].imshow(sample['pred_mask'], cmap='hot', vmin=0, vmax=1)\n",
    "        axes[2, i].set_title(f'Pred Prob {i+1}')\n",
    "        axes[2, i].axis('off')\n",
    "        \n",
    "        # Predicted mask (binary)\n",
    "        axes[3, i].imshow(sample['pred_binary'], cmap='gray')\n",
    "        axes[3, i].set_title(f'Pred Binary {i+1}')\n",
    "        axes[3, i].axis('off')\n",
    "    \n",
    "    plt.suptitle('DeepLabV3+ ResNet - Prediction Results', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate model on samples\n",
    "print(\"üîç ƒê√°nh gi√° model tr√™n validation samples:\")\n",
    "evaluate_model_on_samples(model, val_loader, num_samples=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final results\n",
    "print(\"üéØ DEEPLABV3+ RESNET TRAINING RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìä Final Metrics (Last Epoch):\")\n",
    "print(f\"   - Training Loss: {deeplabv3_resnet_history['train_losses'][-1]:.4f}\")\n",
    "print(f\"   - Validation Loss: {deeplabv3_resnet_history['val_losses'][-1]:.4f}\")\n",
    "print(f\"   - Training Dice: {deeplabv3_resnet_history['train_dice'][-1]:.4f}\")\n",
    "print(f\"   - Validation Dice: {deeplabv3_resnet_history['val_dice'][-1]:.4f}\")\n",
    "print(f\"   - Training Jaccard: {deeplabv3_resnet_history['train_jaccard'][-1]:.4f}\")\n",
    "print(f\"   - Validation Jaccard: {deeplabv3_resnet_history['val_jaccard'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Metrics:\")\n",
    "best_val_dice_idx = np.argmax(deeplabv3_resnet_history['val_dice'])\n",
    "best_val_jaccard_idx = np.argmax(deeplabv3_resnet_history['val_jaccard'])\n",
    "print(f\"   - Best Validation Dice: {max(deeplabv3_resnet_history['val_dice']):.4f} (Epoch {best_val_dice_idx + 1})\")\n",
    "print(f\"   - Best Validation Jaccard: {max(deeplabv3_resnet_history['val_jaccard']):.4f} (Epoch {best_val_jaccard_idx + 1})\")\n",
    "\n",
    "print(f\"\\nüíæ Saved Models:\")\n",
    "print(f\"   - models/deeplabv3_resnet_model_best.pth\")\n",
    "print(f\"   - models/deeplabv3_resnet_model_final.pth\")\n",
    "\n",
    "print(f\"\\nüìà Model Performance:\")\n",
    "final_dice = deeplabv3_resnet_history['val_dice'][-1]\n",
    "final_jaccard = deeplabv3_resnet_history['val_jaccard'][-1]\n",
    "\n",
    "if final_dice > 0.85:\n",
    "    print(f\"   ‚úÖ Excellent performance! Dice > 0.85\")\n",
    "elif final_dice > 0.80:\n",
    "    print(f\"   ‚úÖ Good performance! Dice > 0.80\")\n",
    "elif final_dice > 0.75:\n",
    "    print(f\"   ‚ö†Ô∏è  Acceptable performance. Dice > 0.75\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Performance needs improvement. Dice < 0.75\")\n",
    "\n",
    "print(f\"\\nüéâ ALL MODELS TRAINING COMPLETED!\")\n",
    "print(f\"\\nüìä Trained Models Summary:\")\n",
    "print(f\"   1. ‚úÖ SegFormer (Transformer-based) - models/segformer_model_*.pth\")\n",
    "print(f\"   2. ‚úÖ U-Net EfficientNet (CNN-based) - models/unet_efficientnet_model_*.pth\")\n",
    "print(f\"   3. ‚úÖ U-Net ViT (Hybrid) - models/unet_vit_model_*.pth\")\n",
    "print(f\"   4. ‚úÖ DeepLabV3+ ResNet (Multi-scale) - models/deeplabv3_resnet_model_*.pth\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(f\"   1. Compare all models' performance\")\n",
    "print(f\"   2. Create ensemble predictions\")\n",
    "print(f\"   3. Evaluate on test set\")\n",
    "print(f\"   4. Deploy best performing model\")\n",
    "\n",
    "print(\"\\n‚úÖ DeepLabV3+ ResNet training completed successfully!\")\n",
    "print(\"üéä Congratulations! All 4 models have been trained successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
