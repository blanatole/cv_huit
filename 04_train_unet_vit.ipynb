{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ Training U-Net v·ªõi Vision Transformer Backbone\n",
    "\n",
    "Notebook n√†y th·ª±c hi·ªán training model **U-Net v·ªõi Vision Transformer backbone** cho b√†i to√°n ph√¢n ƒëo·∫°n kh·ªëi u da:\n",
    "\n",
    "## üéØ Model: U-Net + Vision Transformer\n",
    "- **Architecture**: U-Net v·ªõi Vision Transformer encoder\n",
    "- **Backbone**: Vision Transformer Base (`vit_base_patch16_224`)\n",
    "- **Parameters**: ~90M\n",
    "- **∆Øu ƒëi·ªÉm**: K·∫øt h·ª£p ∆∞u ƒëi·ªÉm c·ªßa ViT v√† U-Net architecture, attention mechanism m·∫°nh m·∫Ω\n",
    "- **Learning Rate Scheduler**: Cosine Annealing v·ªõi warmup\n",
    "\n",
    "## üìä Training Configuration:\n",
    "- **Epochs**: 20\n",
    "- **Learning Rate**: 1e-4\n",
    "- **Batch Size**: 8\n",
    "- **Optimizer**: Adam v·ªõi weight decay 1e-4\n",
    "- **Loss Function**: Combined Loss (BCE + Dice)\n",
    "- **Metrics**: Dice Coefficient, Jaccard Index (IoU)\n",
    "- **Scheduler**: Cosine Annealing v·ªõi warmup (5 epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import th∆∞ vi·ªán v√† setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "!pip install segmentation-models-pytorch timm torch torchvision\n",
    "!pip install transformers einops\n",
    "!pip install albumentations opencv-python-headless\n",
    "!pip install matplotlib seaborn scikit-learn pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Segmentation models\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# Timm for ViT backbone\n",
    "import timm\n",
    "\n",
    "# Transformers for ViT\n",
    "from transformers import ViTModel, ViTConfig\n",
    "\n",
    "# Einops for tensor operations\n",
    "import einops\n",
    "\n",
    "# Albumentations for augmentation\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import jaccard_score, f1_score\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c models n·∫øu ch∆∞a c√≥\n",
    "os.makedirs('models', exist_ok=True)\n",
    "print(\"‚úÖ Setup ho√†n t·∫•t!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset v√† Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISICDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None, target_size=(512, 512)):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        # L·∫•y danh s√°ch file images\n",
    "        self.image_files = sorted([f for f in os.listdir(images_dir) \n",
    "                                 if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        \n",
    "        print(f\"Found {len(self.image_files)} images in {images_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Find corresponding mask\n",
    "        base_name = os.path.splitext(img_name)[0]\n",
    "        possible_mask_names = [\n",
    "            f\"{base_name}_segmentation.png\",\n",
    "            f\"{base_name}_mask.png\",\n",
    "            f\"{base_name}.png\"\n",
    "        ]\n",
    "        \n",
    "        mask = None\n",
    "        for mask_name in possible_mask_names:\n",
    "            mask_path = os.path.join(self.masks_dir, mask_name)\n",
    "            if os.path.exists(mask_path):\n",
    "                mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "                break\n",
    "        \n",
    "        if mask is None:\n",
    "            # Create dummy mask if not found\n",
    "            mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "        \n",
    "        # Resize\n",
    "        image = cv2.resize(image, self.target_size)\n",
    "        mask = cv2.resize(mask, self.target_size)\n",
    "        \n",
    "        # Convert mask to binary\n",
    "        mask = (mask > 127).astype(np.float32)\n",
    "        \n",
    "        # Apply augmentations\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        \n",
    "        # Convert to tensor if not already\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n",
    "        \n",
    "        if not isinstance(mask, torch.Tensor):\n",
    "            mask = torch.from_numpy(mask).float().unsqueeze(0)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# Define augmentations\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Dataset class v√† augmentations ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = ISICDataset(\n",
    "    images_dir='data/train/images',\n",
    "    masks_dir='data/train/ground_truth',\n",
    "    transform=train_transform,\n",
    "    target_size=(512, 512)\n",
    ")\n",
    "\n",
    "val_dataset = ISICDataset(\n",
    "    images_dir='data/val/images',\n",
    "    masks_dir='data/val/ground_truth',\n",
    "    transform=val_transform,\n",
    "    target_size=(512, 512)\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"üìä Dataset Summary:\")\n",
    "print(f\"   - Training samples: {len(train_dataset)}\")\n",
    "print(f\"   - Validation samples: {len(val_dataset)}\")\n",
    "print(f\"   - Batch size: {batch_size}\")\n",
    "print(f\"   - Training batches: {len(train_loader)}\")\n",
    "print(f\"   - Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. U-Net ViT Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetViT(nn.Module):\n",
    "    def __init__(self, backbone_name=\"vit_base_patch16_224\", num_classes=1, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # S·ª≠ d·ª•ng segmentation_models_pytorch ƒë·ªÉ t·∫°o U-Net v·ªõi ViT backbone\n",
    "        # L∆∞u √Ω: ViT backbone c√≥ th·ªÉ kh√¥ng c√≥ s·∫µn trong smp, n√™n ta s·∫Ω d√πng timm backbone\n",
    "        try:\n",
    "            self.model = smp.Unet(\n",
    "                encoder_name=backbone_name,\n",
    "                encoder_weights=\"imagenet\" if pretrained else None,\n",
    "                in_channels=3,\n",
    "                classes=num_classes,\n",
    "                activation=None  # We'll apply sigmoid manually\n",
    "            )\n",
    "            print(f\"‚úÖ U-Net ViT model created with smp:\")\n",
    "        except:\n",
    "            # Fallback: s·ª≠ d·ª•ng ResNet backbone n·∫øu ViT kh√¥ng c√≥ s·∫µn\n",
    "            print(\"‚ö†Ô∏è  ViT backbone kh√¥ng c√≥ s·∫µn trong smp, s·ª≠ d·ª•ng ResNet34 thay th·∫ø\")\n",
    "            self.model = smp.Unet(\n",
    "                encoder_name=\"resnet34\",\n",
    "                encoder_weights=\"imagenet\" if pretrained else None,\n",
    "                in_channels=3,\n",
    "                classes=num_classes,\n",
    "                activation=None\n",
    "            )\n",
    "            backbone_name = \"resnet34 (fallback)\"\n",
    "        \n",
    "        print(f\"   - Backbone: {backbone_name}\")\n",
    "        print(f\"   - Pre-trained: {pretrained}\")\n",
    "        print(f\"   - Number of classes: {num_classes}\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through U-Net\n",
    "        logits = self.model(x)\n",
    "        \n",
    "        # Apply sigmoid for binary segmentation\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "# Initialize model\n",
    "print(\"üî¨ Initializing U-Net ViT model...\")\n",
    "model = UNetViT(\n",
    "    backbone_name=\"vit_base_patch16_224\",\n",
    "    num_classes=1,\n",
    "    pretrained=True\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"üìä Model Statistics:\")\n",
    "print(f\"   - Total parameters: {total_params:,}\")\n",
    "print(f\"   - Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   - Model size: ~{total_params/1e6:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Functions v√† Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.view(-1)\n",
    "        target = target.view(-1)\n",
    "        \n",
    "        intersection = (pred * target).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)\n",
    "        \n",
    "        return 1 - dice\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.dice = DiceLoss()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        return self.alpha * self.bce(pred, target) + (1 - self.alpha) * self.dice(pred, target)\n",
    "\n",
    "def calculate_dice_batch(pred, target, threshold=0.5):\n",
    "    \"\"\"Calculate Dice coefficient for a batch\"\"\"\n",
    "    pred_binary = (pred > threshold).float()\n",
    "    target_binary = target.float()\n",
    "    \n",
    "    intersection = (pred_binary * target_binary).sum()\n",
    "    dice = (2. * intersection) / (pred_binary.sum() + target_binary.sum() + 1e-6)\n",
    "    \n",
    "    return dice.item()\n",
    "\n",
    "def calculate_jaccard_batch(pred, target):\n",
    "    \"\"\"Calculate Jaccard Index (IoU) for a batch\"\"\"\n",
    "    intersection = (pred & target).float().sum()\n",
    "    union = (pred | target).float().sum()\n",
    "    \n",
    "    jaccard = intersection / (union + 1e-6)\n",
    "    return jaccard.item()\n",
    "\n",
    "print(\"‚úÖ Loss functions v√† metrics ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cosine Annealing v·ªõi Warmup Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineAnnealingWarmupRestarts(torch.optim.lr_scheduler._LRScheduler):\n",
    "    \"\"\"\n",
    "    Cosine Annealing v·ªõi Warmup scheduler\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, first_cycle_steps, cycle_mult=1., max_lr=0.1, min_lr=0.001, \n",
    "                 warmup_steps=0, gamma=1., last_epoch=-1):\n",
    "        self.first_cycle_steps = first_cycle_steps\n",
    "        self.cycle_mult = cycle_mult\n",
    "        self.base_max_lr = max_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.cur_cycle_steps = first_cycle_steps\n",
    "        self.cycle = 0\n",
    "        self.step_in_cycle = last_epoch\n",
    "        \n",
    "        super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)\n",
    "        \n",
    "        # Set learning rate to min_lr\n",
    "        self.init_lr()\n",
    "    \n",
    "    def init_lr(self):\n",
    "        self.base_lrs = []\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = self.min_lr\n",
    "            self.base_lrs.append(self.min_lr)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.step_in_cycle == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.step_in_cycle < self.warmup_steps:\n",
    "            return [(self.max_lr - base_lr)*self.step_in_cycle / self.warmup_steps + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.max_lr - base_lr) * \\\n",
    "                    (1 + np.cos(np.pi * (self.step_in_cycle-self.warmup_steps) / (self.cur_cycle_steps - self.warmup_steps))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.step_in_cycle = self.step_in_cycle + 1\n",
    "            if self.step_in_cycle >= self.cur_cycle_steps:\n",
    "                self.cycle += 1\n",
    "                self.step_in_cycle = self.step_in_cycle - self.cur_cycle_steps\n",
    "                self.cur_cycle_steps = int((self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\n",
    "        else:\n",
    "            if epoch >= self.first_cycle_steps:\n",
    "                if self.cycle_mult == 1.:\n",
    "                    self.step_in_cycle = epoch % self.first_cycle_steps\n",
    "                    self.cycle = epoch // self.first_cycle_steps\n",
    "                else:\n",
    "                    n = int(np.log((epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1), self.cycle_mult))\n",
    "                    self.cycle = n\n",
    "                    self.step_in_cycle = epoch - int(self.first_cycle_steps * (self.cycle_mult ** n - 1) / (self.cycle_mult - 1))\n",
    "                    self.cur_cycle_steps = self.first_cycle_steps * self.cycle_mult ** (n)\n",
    "            else:\n",
    "                self.cur_cycle_steps = self.first_cycle_steps\n",
    "                self.step_in_cycle = epoch\n",
    "                \n",
    "        self.max_lr = self.base_max_lr * (self.gamma**self.cycle)\n",
    "        self.last_epoch = epoch\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "print(\"‚úÖ Cosine Annealing Warmup scheduler ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unet_vit(model, train_loader, val_loader, num_epochs=20, lr=1e-4, warmup_epochs=5):\n",
    "    \"\"\"\n",
    "    Training function for U-Net ViT with Cosine Annealing Warmup scheduler\n",
    "    \"\"\"\n",
    "    # Loss function and optimizer\n",
    "    criterion = CombinedLoss(alpha=0.5)  # Combination of BCE and Dice loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    \n",
    "    # Cosine Annealing v·ªõi Warmup scheduler\n",
    "    scheduler = CosineAnnealingWarmupRestarts(\n",
    "        optimizer,\n",
    "        first_cycle_steps=num_epochs,\n",
    "        max_lr=lr,\n",
    "        min_lr=lr*0.01,\n",
    "        warmup_steps=warmup_epochs\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_losses': [],\n",
    "        'val_losses': [],\n",
    "        'train_dice': [],\n",
    "        'val_dice': [],\n",
    "        'train_jaccard': [],\n",
    "        'val_jaccard': [],\n",
    "        'learning_rates': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    print(f\"üöÄ B·∫Øt ƒë·∫ßu training U-Net ViT...\")\n",
    "    print(f\"üìä Configuration:\")\n",
    "    print(f\"   - Epochs: {num_epochs}\")\n",
    "    print(f\"   - Learning Rate: {lr}\")\n",
    "    print(f\"   - Warmup Epochs: {warmup_epochs}\")\n",
    "    print(f\"   - Scheduler: Cosine Annealing v·ªõi Warmup\")\n",
    "    print(f\"   - Loss: Combined (BCE + Dice)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_dice = 0.0\n",
    "        train_jaccard = 0.0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Training')\n",
    "        for batch_idx, (images, masks) in enumerate(train_pbar):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            with torch.no_grad():\n",
    "                pred_masks = outputs > 0.5\n",
    "                dice = calculate_dice_batch(outputs, masks)\n",
    "                jaccard = calculate_jaccard_batch(pred_masks, masks.bool())\n",
    "                train_dice += dice\n",
    "                train_jaccard += jaccard\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}', \n",
    "                'Dice': f'{dice:.3f}', \n",
    "                'Jaccard': f'{jaccard:.3f}'\n",
    "            })\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_dice = 0.0\n",
    "        val_jaccard = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Validation')\n",
    "            for images, masks in val_pbar:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                pred_masks = outputs > 0.5\n",
    "                dice = calculate_dice_batch(outputs, masks)\n",
    "                jaccard = calculate_jaccard_batch(pred_masks, masks.bool())\n",
    "                val_dice += dice\n",
    "                val_jaccard += jaccard\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_pbar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}', \n",
    "                    'Dice': f'{dice:.3f}', \n",
    "                    'Jaccard': f'{jaccard:.3f}'\n",
    "                })\n",
    "        \n",
    "        # Calculate averages\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        train_dice /= len(train_loader)\n",
    "        val_dice /= len(val_loader)\n",
    "        train_jaccard /= len(train_loader)\n",
    "        val_jaccard /= len(val_loader)\n",
    "        \n",
    "        # Update learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step()\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Store history\n",
    "        history['train_losses'].append(train_loss)\n",
    "        history['val_losses'].append(val_loss)\n",
    "        history['train_dice'].append(train_dice)\n",
    "        history['val_dice'].append(val_dice)\n",
    "        history['train_jaccard'].append(train_jaccard)\n",
    "        history['val_jaccard'].append(val_jaccard)\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        if abs(current_lr - new_lr) > 1e-8:\n",
    "            print(f\"  Learning rate changed: {current_lr:.6f} -> {new_lr:.6f}\")\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Dice: {train_dice:.4f}, Train Jaccard: {train_jaccard:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Dice: {val_dice:.4f}, Val Jaccard: {val_jaccard:.4f}\")\n",
    "        print(f\"  LR: {current_lr:.6f}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'models/unet_vit_model_best.pth')\n",
    "            print(f\"New best validation loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"‚úÖ Training function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. B·∫Øt ƒë·∫ßu Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the U-Net ViT model\n",
    "print(\"üöÄ B·∫Øt ƒë·∫ßu training U-Net ViT...\")\n",
    "print(\"‚è∞ Th·ªùi gian d·ª± ki·∫øn: ~2-3 gi·ªù (t√πy thu·ªôc v√†o GPU)\")\n",
    "print()\n",
    "\n",
    "unet_vit_history = train_unet_vit(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=20,\n",
    "    lr=1e-4,\n",
    "    warmup_epochs=5\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), 'models/unet_vit_model_final.pth')\n",
    "print(\"\\nüéâ U-Net ViT training ho√†n th√†nh!\")\n",
    "print(\"üíæ Model ƒë√£ ƒë∆∞·ª£c l∆∞u:\")\n",
    "print(\"   - models/unet_vit_model_best.pth (best validation loss)\")\n",
    "print(\"   - models/unet_vit_model_final.pth (final epoch)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization v√† Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, model_name=\"U-Net ViT\"):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(history['train_losses'], label='Train Loss', color='blue')\n",
    "    axes[0, 0].plot(history['val_losses'], label='Val Loss', color='red')\n",
    "    axes[0, 0].set_title(f'{model_name} - Training & Validation Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Dice Coefficient\n",
    "    axes[0, 1].plot(history['train_dice'], label='Train Dice', color='blue')\n",
    "    axes[0, 1].plot(history['val_dice'], label='Val Dice', color='red')\n",
    "    axes[0, 1].set_title(f'{model_name} - Dice Coefficient')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Dice Score')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Jaccard Index\n",
    "    axes[1, 0].plot(history['train_jaccard'], label='Train Jaccard', color='blue')\n",
    "    axes[1, 0].plot(history['val_jaccard'], label='Val Jaccard', color='red')\n",
    "    axes[1, 0].set_title(f'{model_name} - Jaccard Index (IoU)')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Jaccard Score')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # Learning Rate\n",
    "    axes[1, 1].plot(history['learning_rates'], label='Learning Rate', color='green')\n",
    "    axes[1, 1].set_title(f'{model_name} - Learning Rate Schedule (Cosine + Warmup)')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Learning Rate')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "print(\"üìä Hi·ªÉn th·ªã training history:\")\n",
    "plot_training_history(unet_vit_history, \"U-Net ViT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_samples(model, val_loader, num_samples=6):\n",
    "    \"\"\"Evaluate model on sample images\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get some samples\n",
    "    samples = []\n",
    "    with torch.no_grad():\n",
    "        for i, (images, masks) in enumerate(val_loader):\n",
    "            if i >= num_samples // val_loader.batch_size + 1:\n",
    "                break\n",
    "                \n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            for j in range(min(images.shape[0], num_samples - len(samples))):\n",
    "                # Denormalize image for visualization\n",
    "                img = images[j].cpu()\n",
    "                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "                img = img * std + mean\n",
    "                img = torch.clamp(img, 0, 1)\n",
    "                \n",
    "                samples.append({\n",
    "                    'image': img.permute(1, 2, 0).numpy(),\n",
    "                    'true_mask': masks[j, 0].cpu().numpy(),\n",
    "                    'pred_mask': outputs[j, 0].cpu().numpy(),\n",
    "                    'pred_binary': (outputs[j, 0] > 0.5).cpu().numpy().astype(np.uint8)\n",
    "                })\n",
    "                \n",
    "                if len(samples) >= num_samples:\n",
    "                    break\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, axes = plt.subplots(4, len(samples), figsize=(20, 16))\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        # Original image\n",
    "        axes[0, i].imshow(sample['image'])\n",
    "        axes[0, i].set_title(f'Original {i+1}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # True mask\n",
    "        axes[1, i].imshow(sample['true_mask'], cmap='gray')\n",
    "        axes[1, i].set_title(f'True Mask {i+1}')\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # Predicted mask (probability)\n",
    "        axes[2, i].imshow(sample['pred_mask'], cmap='hot', vmin=0, vmax=1)\n",
    "        axes[2, i].set_title(f'Pred Prob {i+1}')\n",
    "        axes[2, i].axis('off')\n",
    "        \n",
    "        # Predicted mask (binary)\n",
    "        axes[3, i].imshow(sample['pred_binary'], cmap='gray')\n",
    "        axes[3, i].set_title(f'Pred Binary {i+1}')\n",
    "        axes[3, i].axis('off')\n",
    "    \n",
    "    plt.suptitle('U-Net ViT - Prediction Results', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate model on samples\n",
    "print(\"üîç ƒê√°nh gi√° model tr√™n validation samples:\")\n",
    "evaluate_model_on_samples(model, val_loader, num_samples=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final results\n",
    "print(\"üéØ U-NET VIT TRAINING RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìä Final Metrics (Last Epoch):\")\n",
    "print(f\"   - Training Loss: {unet_vit_history['train_losses'][-1]:.4f}\")\n",
    "print(f\"   - Validation Loss: {unet_vit_history['val_losses'][-1]:.4f}\")\n",
    "print(f\"   - Training Dice: {unet_vit_history['train_dice'][-1]:.4f}\")\n",
    "print(f\"   - Validation Dice: {unet_vit_history['val_dice'][-1]:.4f}\")\n",
    "print(f\"   - Training Jaccard: {unet_vit_history['train_jaccard'][-1]:.4f}\")\n",
    "print(f\"   - Validation Jaccard: {unet_vit_history['val_jaccard'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Metrics:\")\n",
    "best_val_dice_idx = np.argmax(unet_vit_history['val_dice'])\n",
    "best_val_jaccard_idx = np.argmax(unet_vit_history['val_jaccard'])\n",
    "print(f\"   - Best Validation Dice: {max(unet_vit_history['val_dice']):.4f} (Epoch {best_val_dice_idx + 1})\")\n",
    "print(f\"   - Best Validation Jaccard: {max(unet_vit_history['val_jaccard']):.4f} (Epoch {best_val_jaccard_idx + 1})\")\n",
    "\n",
    "print(f\"\\nüíæ Saved Models:\")\n",
    "print(f\"   - models/unet_vit_model_best.pth\")\n",
    "print(f\"   - models/unet_vit_model_final.pth\")\n",
    "\n",
    "print(f\"\\nüìà Model Performance:\")\n",
    "final_dice = unet_vit_history['val_dice'][-1]\n",
    "final_jaccard = unet_vit_history['val_jaccard'][-1]\n",
    "\n",
    "if final_dice > 0.85:\n",
    "    print(f\"   ‚úÖ Excellent performance! Dice > 0.85\")\n",
    "elif final_dice > 0.80:\n",
    "    print(f\"   ‚úÖ Good performance! Dice > 0.80\")\n",
    "elif final_dice > 0.75:\n",
    "    print(f\"   ‚ö†Ô∏è  Acceptable performance. Dice > 0.75\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Performance needs improvement. Dice < 0.75\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(f\"   1. Run 05_train_deeplabv3_resnet.ipynb to train DeepLabV3+ ResNet\")\n",
    "print(f\"   2. Compare all models' performance\")\n",
    "print(f\"   3. Create ensemble predictions\")\n",
    "\n",
    "print(\"\\n‚úÖ U-Net ViT training completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
