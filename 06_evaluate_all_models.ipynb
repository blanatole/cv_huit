{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧪 Đánh giá tất cả Models trên Test Set\n",
    "\n",
    "Notebook này thực hiện đánh giá tất cả các models đã train trên test set:\n",
    "\n",
    "## 🎯 Models được đánh giá:\n",
    "- **SegFormer**: Transformer-based segmentation\n",
    "- **U-Net EfficientNet**: CNN với EfficientNet backbone\n",
    "- **ViT Segmentation**: Pure Vision Transformer\n",
    "- **DeepLabV3+ ResNet**: Atrous convolution với ResNet\n",
    "\n",
    "## 📊 Metrics:\n",
    "- **Dice Coefficient**: Overlap measure\n",
    "- **Jaccard Index (IoU)**: Intersection over Union\n",
    "- **Test Loss**: Combined BCE + Dice loss\n",
    "- **Statistical Analysis**: Mean, std, distribution\n",
    "\n",
    "## 📈 Output:\n",
    "- **JSON Results**: Detailed metrics cho từng model\n",
    "- **Comparison Table**: So sánh performance\n",
    "- **Visualizations**: Sample predictions và charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep learning libraries\n",
    "import timm\n",
    "import segmentation_models_pytorch as smp\n",
    "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
    "\n",
    "# Data loading\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 5070 Ti\n",
      "Memory: 15.5 GB\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test dataset loaded: 1000 samples\n",
      "   - Test batches: 250\n"
     ]
    }
   ],
   "source": [
    "# Dataset class (same as training)\n",
    "class SkinLesionDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Get all image files\n",
    "        self.image_files = [f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.png'))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        # Load mask\n",
    "        mask_name = img_name.replace('.jpg', '_segmentation.png')\n",
    "        mask_path = os.path.join(self.masks_dir, mask_name)\n",
    "        mask = np.array(Image.open(mask_path).convert('L'))\n",
    "        mask = (mask > 127).astype(np.float32)  # Binary threshold\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# Test transforms (same as training)\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = SkinLesionDataset(\n",
    "    images_dir='data/test/images',\n",
    "    masks_dir='data/test/masks',\n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"✅ Test dataset loaded: {len(test_dataset)} samples\")\n",
    "print(f\"   - Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loss functions và metrics defined!\n"
     ]
    }
   ],
   "source": [
    "# Loss functions và metrics (consistent với training)\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.contiguous().view(-1)\n",
    "        target = target.contiguous().view(-1)\n",
    "        \n",
    "        intersection = (pred * target).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)\n",
    "        \n",
    "        return 1 - dice\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.bce = nn.BCEWithLogitsLoss()  # Safe for both logits and probabilities\n",
    "        self.dice = DiceLoss()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        # Handle shape differences\n",
    "        if pred.dim() == 4 and target.dim() == 3:\n",
    "            pred = pred.squeeze(1)\n",
    "        elif pred.dim() == 3 and target.dim() == 4:\n",
    "            target = target.squeeze(1)\n",
    "        \n",
    "        # Handle both logits and probabilities\n",
    "        if pred.min() < 0 or pred.max() > 1:\n",
    "            # Logits: use directly for BCE, convert for Dice\n",
    "            bce_loss = self.bce(pred, target)\n",
    "            dice_pred = torch.sigmoid(pred)\n",
    "            dice_loss = self.dice(dice_pred, target)\n",
    "        else:\n",
    "            # Probabilities: convert to logits for BCE\n",
    "            pred_logits = torch.logit(pred.clamp(1e-7, 1-1e-7))\n",
    "            bce_loss = self.bce(pred_logits, target)\n",
    "            dice_loss = self.dice(pred, target)\n",
    "        \n",
    "        return self.alpha * bce_loss + (1 - self.alpha) * dice_loss\n",
    "\n",
    "def calculate_dice_batch(pred, target, threshold=0.5):\n",
    "    \"\"\"Calculate Dice coefficient for a batch\"\"\"\n",
    "    # Convert to probabilities if needed\n",
    "    if pred.min() < 0 or pred.max() > 1:\n",
    "        pred = torch.sigmoid(pred)\n",
    "    \n",
    "    # Handle shape differences\n",
    "    if pred.dim() == 4 and target.dim() == 3:\n",
    "        pred = pred.squeeze(1)\n",
    "    elif pred.dim() == 3 and target.dim() == 4:\n",
    "        target = target.squeeze(1)\n",
    "    \n",
    "    pred_binary = (pred > threshold).float()\n",
    "    target_binary = target.float()\n",
    "    \n",
    "    intersection = (pred_binary * target_binary).sum()\n",
    "    dice = (2. * intersection) / (pred_binary.sum() + target_binary.sum() + 1e-6)\n",
    "    \n",
    "    return dice.item()\n",
    "\n",
    "def calculate_jaccard_batch(pred, target, threshold=0.5):\n",
    "    \"\"\"Calculate Jaccard Index (IoU) for a batch\"\"\"\n",
    "    # Convert to probabilities if needed\n",
    "    if pred.min() < 0 or pred.max() > 1:\n",
    "        pred = torch.sigmoid(pred)\n",
    "    \n",
    "    # Handle shape differences\n",
    "    if pred.dim() == 4 and target.dim() == 3:\n",
    "        pred = pred.squeeze(1)\n",
    "    elif pred.dim() == 3 and target.dim() == 4:\n",
    "        target = target.squeeze(1)\n",
    "    \n",
    "    pred_bool = (pred > threshold).bool()\n",
    "    target_bool = target.bool()\n",
    "    \n",
    "    intersection = (pred_bool & target_bool).float().sum()\n",
    "    union = (pred_bool | target_bool).float().sum()\n",
    "    \n",
    "    jaccard = intersection / (union + 1e-6)\n",
    "    return jaccard.item()\n",
    "\n",
    "print(\"✅ Loss functions và metrics defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation function defined!\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, model_name='Model'):\n",
    "    \"\"\"Comprehensive evaluation on test set\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    all_dice_scores = []\n",
    "    all_jaccard_scores = []\n",
    "    \n",
    "    criterion = CombinedLoss(alpha=0.5)\n",
    "    \n",
    "    print(f'🧪 Evaluating {model_name} on test set...')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_pbar = tqdm(test_loader, desc=f'Testing {model_name}')\n",
    "        for images, masks in test_pbar:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate metrics for each sample in batch\n",
    "            for i in range(images.shape[0]):\n",
    "                # Extract single samples\n",
    "                mask_sample = masks[i:i+1]\n",
    "                output_sample = outputs[i:i+1]\n",
    "                \n",
    "                dice = calculate_dice_batch(output_sample, mask_sample)\n",
    "                jaccard = calculate_jaccard_batch(output_sample, mask_sample)\n",
    "                \n",
    "                all_dice_scores.append(dice)\n",
    "                all_jaccard_scores.append(jaccard)\n",
    "            \n",
    "            # Update progress bar\n",
    "            test_pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Dice': f'{np.mean(all_dice_scores[-images.shape[0]:]):.4f}',\n",
    "                'IoU': f'{np.mean(all_jaccard_scores[-images.shape[0]:]):.4f}'\n",
    "            })\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    avg_dice = np.mean(all_dice_scores)\n",
    "    std_dice = np.std(all_dice_scores)\n",
    "    avg_jaccard = np.mean(all_jaccard_scores)\n",
    "    std_jaccard = np.std(all_jaccard_scores)\n",
    "    \n",
    "    # Print results\n",
    "    print(f'\\n📊 {model_name} Test Results:')\n",
    "    print(f'   - Test Loss: {avg_test_loss:.4f}')\n",
    "    print(f'   - Dice Score: {avg_dice:.4f} ± {std_dice:.4f}')\n",
    "    print(f'   - Jaccard (IoU): {avg_jaccard:.4f} ± {std_jaccard:.4f}')\n",
    "    print(f'   - Samples evaluated: {len(all_dice_scores)}')\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'test_loss': avg_test_loss,\n",
    "        'dice_mean': avg_dice,\n",
    "        'dice_std': std_dice,\n",
    "        'jaccard_mean': avg_jaccard,\n",
    "        'jaccard_std': std_jaccard,\n",
    "        'dice_scores': all_dice_scores,\n",
    "        'jaccard_scores': all_jaccard_scores,\n",
    "        'num_samples': len(all_dice_scores)\n",
    "    }\n",
    "\n",
    "print(\"✅ Evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Model 1: SegFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔍 EVALUATING SEGFORMER MODEL\n",
      "==================================================\n",
      "🔄 Loading SegFormer model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([1, 256, 1, 1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SegformerForSemanticSegmentation:\n\tMissing key(s) in state_dict: \"segformer.encoder.patch_embeddings.0.proj.weight\", \"segformer.encoder.patch_embeddings.0.proj.bias\", \"segformer.encoder.patch_embeddings.0.layer_norm.weight\", \"segformer.encoder.patch_embeddings.0.layer_norm.bias\", \"segformer.encoder.patch_embeddings.1.proj.weight\", \"segformer.encoder.patch_embeddings.1.proj.bias\", \"segformer.encoder.patch_embeddings.1.layer_norm.weight\", \"segformer.encoder.patch_embeddings.1.layer_norm.bias\", \"segformer.encoder.patch_embeddings.2.proj.weight\", \"segformer.encoder.patch_embeddings.2.proj.bias\", \"segformer.encoder.patch_embeddings.2.layer_norm.weight\", \"segformer.encoder.patch_embeddings.2.layer_norm.bias\", \"segformer.encoder.patch_embeddings.3.proj.weight\", \"segformer.encoder.patch_embeddings.3.proj.bias\", \"segformer.encoder.patch_embeddings.3.layer_norm.weight\", \"segformer.encoder.patch_embeddings.3.layer_norm.bias\", \"segformer.encoder.block.0.0.layer_norm_1.weight\", \"segformer.encoder.block.0.0.layer_norm_1.bias\", \"segformer.encoder.block.0.0.attention.self.query.weight\", \"segformer.encoder.block.0.0.attention.self.query.bias\", \"segformer.encoder.block.0.0.attention.self.key.weight\", \"segformer.encoder.block.0.0.attention.self.key.bias\", \"segformer.encoder.block.0.0.attention.self.value.weight\", \"segformer.encoder.block.0.0.attention.self.value.bias\", \"segformer.encoder.block.0.0.attention.self.sr.weight\", \"segformer.encoder.block.0.0.attention.self.sr.bias\", \"segformer.encoder.block.0.0.attention.self.layer_norm.weight\", \"segformer.encoder.block.0.0.attention.self.layer_norm.bias\", \"segformer.encoder.block.0.0.attention.output.dense.weight\", \"segformer.encoder.block.0.0.attention.output.dense.bias\", \"segformer.encoder.block.0.0.layer_norm_2.weight\", \"segformer.encoder.block.0.0.layer_norm_2.bias\", \"segformer.encoder.block.0.0.mlp.dense1.weight\", \"segformer.encoder.block.0.0.mlp.dense1.bias\", \"segformer.encoder.block.0.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.0.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.0.0.mlp.dense2.weight\", \"segformer.encoder.block.0.0.mlp.dense2.bias\", \"segformer.encoder.block.0.1.layer_norm_1.weight\", \"segformer.encoder.block.0.1.layer_norm_1.bias\", \"segformer.encoder.block.0.1.attention.self.query.weight\", \"segformer.encoder.block.0.1.attention.self.query.bias\", \"segformer.encoder.block.0.1.attention.self.key.weight\", \"segformer.encoder.block.0.1.attention.self.key.bias\", \"segformer.encoder.block.0.1.attention.self.value.weight\", \"segformer.encoder.block.0.1.attention.self.value.bias\", \"segformer.encoder.block.0.1.attention.self.sr.weight\", \"segformer.encoder.block.0.1.attention.self.sr.bias\", \"segformer.encoder.block.0.1.attention.self.layer_norm.weight\", \"segformer.encoder.block.0.1.attention.self.layer_norm.bias\", \"segformer.encoder.block.0.1.attention.output.dense.weight\", \"segformer.encoder.block.0.1.attention.output.dense.bias\", \"segformer.encoder.block.0.1.layer_norm_2.weight\", \"segformer.encoder.block.0.1.layer_norm_2.bias\", \"segformer.encoder.block.0.1.mlp.dense1.weight\", \"segformer.encoder.block.0.1.mlp.dense1.bias\", \"segformer.encoder.block.0.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.0.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.0.1.mlp.dense2.weight\", \"segformer.encoder.block.0.1.mlp.dense2.bias\", \"segformer.encoder.block.1.0.layer_norm_1.weight\", \"segformer.encoder.block.1.0.layer_norm_1.bias\", \"segformer.encoder.block.1.0.attention.self.query.weight\", \"segformer.encoder.block.1.0.attention.self.query.bias\", \"segformer.encoder.block.1.0.attention.self.key.weight\", \"segformer.encoder.block.1.0.attention.self.key.bias\", \"segformer.encoder.block.1.0.attention.self.value.weight\", \"segformer.encoder.block.1.0.attention.self.value.bias\", \"segformer.encoder.block.1.0.attention.self.sr.weight\", \"segformer.encoder.block.1.0.attention.self.sr.bias\", \"segformer.encoder.block.1.0.attention.self.layer_norm.weight\", \"segformer.encoder.block.1.0.attention.self.layer_norm.bias\", \"segformer.encoder.block.1.0.attention.output.dense.weight\", \"segformer.encoder.block.1.0.attention.output.dense.bias\", \"segformer.encoder.block.1.0.layer_norm_2.weight\", \"segformer.encoder.block.1.0.layer_norm_2.bias\", \"segformer.encoder.block.1.0.mlp.dense1.weight\", \"segformer.encoder.block.1.0.mlp.dense1.bias\", \"segformer.encoder.block.1.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.1.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.1.0.mlp.dense2.weight\", \"segformer.encoder.block.1.0.mlp.dense2.bias\", \"segformer.encoder.block.1.1.layer_norm_1.weight\", \"segformer.encoder.block.1.1.layer_norm_1.bias\", \"segformer.encoder.block.1.1.attention.self.query.weight\", \"segformer.encoder.block.1.1.attention.self.query.bias\", \"segformer.encoder.block.1.1.attention.self.key.weight\", \"segformer.encoder.block.1.1.attention.self.key.bias\", \"segformer.encoder.block.1.1.attention.self.value.weight\", \"segformer.encoder.block.1.1.attention.self.value.bias\", \"segformer.encoder.block.1.1.attention.self.sr.weight\", \"segformer.encoder.block.1.1.attention.self.sr.bias\", \"segformer.encoder.block.1.1.attention.self.layer_norm.weight\", \"segformer.encoder.block.1.1.attention.self.layer_norm.bias\", \"segformer.encoder.block.1.1.attention.output.dense.weight\", \"segformer.encoder.block.1.1.attention.output.dense.bias\", \"segformer.encoder.block.1.1.layer_norm_2.weight\", \"segformer.encoder.block.1.1.layer_norm_2.bias\", \"segformer.encoder.block.1.1.mlp.dense1.weight\", \"segformer.encoder.block.1.1.mlp.dense1.bias\", \"segformer.encoder.block.1.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.1.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.1.1.mlp.dense2.weight\", \"segformer.encoder.block.1.1.mlp.dense2.bias\", \"segformer.encoder.block.2.0.layer_norm_1.weight\", \"segformer.encoder.block.2.0.layer_norm_1.bias\", \"segformer.encoder.block.2.0.attention.self.query.weight\", \"segformer.encoder.block.2.0.attention.self.query.bias\", \"segformer.encoder.block.2.0.attention.self.key.weight\", \"segformer.encoder.block.2.0.attention.self.key.bias\", \"segformer.encoder.block.2.0.attention.self.value.weight\", \"segformer.encoder.block.2.0.attention.self.value.bias\", \"segformer.encoder.block.2.0.attention.self.sr.weight\", \"segformer.encoder.block.2.0.attention.self.sr.bias\", \"segformer.encoder.block.2.0.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.0.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.0.attention.output.dense.weight\", \"segformer.encoder.block.2.0.attention.output.dense.bias\", \"segformer.encoder.block.2.0.layer_norm_2.weight\", \"segformer.encoder.block.2.0.layer_norm_2.bias\", \"segformer.encoder.block.2.0.mlp.dense1.weight\", \"segformer.encoder.block.2.0.mlp.dense1.bias\", \"segformer.encoder.block.2.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.0.mlp.dense2.weight\", \"segformer.encoder.block.2.0.mlp.dense2.bias\", \"segformer.encoder.block.2.1.layer_norm_1.weight\", \"segformer.encoder.block.2.1.layer_norm_1.bias\", \"segformer.encoder.block.2.1.attention.self.query.weight\", \"segformer.encoder.block.2.1.attention.self.query.bias\", \"segformer.encoder.block.2.1.attention.self.key.weight\", \"segformer.encoder.block.2.1.attention.self.key.bias\", \"segformer.encoder.block.2.1.attention.self.value.weight\", \"segformer.encoder.block.2.1.attention.self.value.bias\", \"segformer.encoder.block.2.1.attention.self.sr.weight\", \"segformer.encoder.block.2.1.attention.self.sr.bias\", \"segformer.encoder.block.2.1.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.1.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.1.attention.output.dense.weight\", \"segformer.encoder.block.2.1.attention.output.dense.bias\", \"segformer.encoder.block.2.1.layer_norm_2.weight\", \"segformer.encoder.block.2.1.layer_norm_2.bias\", \"segformer.encoder.block.2.1.mlp.dense1.weight\", \"segformer.encoder.block.2.1.mlp.dense1.bias\", \"segformer.encoder.block.2.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.1.mlp.dense2.weight\", \"segformer.encoder.block.2.1.mlp.dense2.bias\", \"segformer.encoder.block.3.0.layer_norm_1.weight\", \"segformer.encoder.block.3.0.layer_norm_1.bias\", \"segformer.encoder.block.3.0.attention.self.query.weight\", \"segformer.encoder.block.3.0.attention.self.query.bias\", \"segformer.encoder.block.3.0.attention.self.key.weight\", \"segformer.encoder.block.3.0.attention.self.key.bias\", \"segformer.encoder.block.3.0.attention.self.value.weight\", \"segformer.encoder.block.3.0.attention.self.value.bias\", \"segformer.encoder.block.3.0.attention.output.dense.weight\", \"segformer.encoder.block.3.0.attention.output.dense.bias\", \"segformer.encoder.block.3.0.layer_norm_2.weight\", \"segformer.encoder.block.3.0.layer_norm_2.bias\", \"segformer.encoder.block.3.0.mlp.dense1.weight\", \"segformer.encoder.block.3.0.mlp.dense1.bias\", \"segformer.encoder.block.3.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.3.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.3.0.mlp.dense2.weight\", \"segformer.encoder.block.3.0.mlp.dense2.bias\", \"segformer.encoder.block.3.1.layer_norm_1.weight\", \"segformer.encoder.block.3.1.layer_norm_1.bias\", \"segformer.encoder.block.3.1.attention.self.query.weight\", \"segformer.encoder.block.3.1.attention.self.query.bias\", \"segformer.encoder.block.3.1.attention.self.key.weight\", \"segformer.encoder.block.3.1.attention.self.key.bias\", \"segformer.encoder.block.3.1.attention.self.value.weight\", \"segformer.encoder.block.3.1.attention.self.value.bias\", \"segformer.encoder.block.3.1.attention.output.dense.weight\", \"segformer.encoder.block.3.1.attention.output.dense.bias\", \"segformer.encoder.block.3.1.layer_norm_2.weight\", \"segformer.encoder.block.3.1.layer_norm_2.bias\", \"segformer.encoder.block.3.1.mlp.dense1.weight\", \"segformer.encoder.block.3.1.mlp.dense1.bias\", \"segformer.encoder.block.3.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.3.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.3.1.mlp.dense2.weight\", \"segformer.encoder.block.3.1.mlp.dense2.bias\", \"segformer.encoder.layer_norm.0.weight\", \"segformer.encoder.layer_norm.0.bias\", \"segformer.encoder.layer_norm.1.weight\", \"segformer.encoder.layer_norm.1.bias\", \"segformer.encoder.layer_norm.2.weight\", \"segformer.encoder.layer_norm.2.bias\", \"segformer.encoder.layer_norm.3.weight\", \"segformer.encoder.layer_norm.3.bias\", \"decode_head.linear_c.0.proj.weight\", \"decode_head.linear_c.0.proj.bias\", \"decode_head.linear_c.1.proj.weight\", \"decode_head.linear_c.1.proj.bias\", \"decode_head.linear_c.2.proj.weight\", \"decode_head.linear_c.2.proj.bias\", \"decode_head.linear_c.3.proj.weight\", \"decode_head.linear_c.3.proj.bias\", \"decode_head.linear_fuse.weight\", \"decode_head.batch_norm.weight\", \"decode_head.batch_norm.bias\", \"decode_head.batch_norm.running_mean\", \"decode_head.batch_norm.running_var\", \"decode_head.classifier.weight\", \"decode_head.classifier.bias\". \n\tUnexpected key(s) in state_dict: \"segformer.segformer.encoder.patch_embeddings.0.proj.weight\", \"segformer.segformer.encoder.patch_embeddings.0.proj.bias\", \"segformer.segformer.encoder.patch_embeddings.0.layer_norm.weight\", \"segformer.segformer.encoder.patch_embeddings.0.layer_norm.bias\", \"segformer.segformer.encoder.patch_embeddings.1.proj.weight\", \"segformer.segformer.encoder.patch_embeddings.1.proj.bias\", \"segformer.segformer.encoder.patch_embeddings.1.layer_norm.weight\", \"segformer.segformer.encoder.patch_embeddings.1.layer_norm.bias\", \"segformer.segformer.encoder.patch_embeddings.2.proj.weight\", \"segformer.segformer.encoder.patch_embeddings.2.proj.bias\", \"segformer.segformer.encoder.patch_embeddings.2.layer_norm.weight\", \"segformer.segformer.encoder.patch_embeddings.2.layer_norm.bias\", \"segformer.segformer.encoder.patch_embeddings.3.proj.weight\", \"segformer.segformer.encoder.patch_embeddings.3.proj.bias\", \"segformer.segformer.encoder.patch_embeddings.3.layer_norm.weight\", \"segformer.segformer.encoder.patch_embeddings.3.layer_norm.bias\", \"segformer.segformer.encoder.block.0.0.layer_norm_1.weight\", \"segformer.segformer.encoder.block.0.0.layer_norm_1.bias\", \"segformer.segformer.encoder.block.0.0.attention.self.query.weight\", \"segformer.segformer.encoder.block.0.0.attention.self.query.bias\", \"segformer.segformer.encoder.block.0.0.attention.self.key.weight\", \"segformer.segformer.encoder.block.0.0.attention.self.key.bias\", \"segformer.segformer.encoder.block.0.0.attention.self.value.weight\", \"segformer.segformer.encoder.block.0.0.attention.self.value.bias\", \"segformer.segformer.encoder.block.0.0.attention.self.sr.weight\", \"segformer.segformer.encoder.block.0.0.attention.self.sr.bias\", \"segformer.segformer.encoder.block.0.0.attention.self.layer_norm.weight\", \"segformer.segformer.encoder.block.0.0.attention.self.layer_norm.bias\", \"segformer.segformer.encoder.block.0.0.attention.output.dense.weight\", \"segformer.segformer.encoder.block.0.0.attention.output.dense.bias\", \"segformer.segformer.encoder.block.0.0.layer_norm_2.weight\", \"segformer.segformer.encoder.block.0.0.layer_norm_2.bias\", \"segformer.segformer.encoder.block.0.0.mlp.dense1.weight\", \"segformer.segformer.encoder.block.0.0.mlp.dense1.bias\", \"segformer.segformer.encoder.block.0.0.mlp.dwconv.dwconv.weight\", \"segformer.segformer.encoder.block.0.0.mlp.dwconv.dwconv.bias\", \"segformer.segformer.encoder.block.0.0.mlp.dense2.weight\", \"segformer.segformer.encoder.block.0.0.mlp.dense2.bias\", \"segformer.segformer.encoder.block.0.1.layer_norm_1.weight\", \"segformer.segformer.encoder.block.0.1.layer_norm_1.bias\", \"segformer.segformer.encoder.block.0.1.attention.self.query.weight\", \"segformer.segformer.encoder.block.0.1.attention.self.query.bias\", \"segformer.segformer.encoder.block.0.1.attention.self.key.weight\", \"segformer.segformer.encoder.block.0.1.attention.self.key.bias\", \"segformer.segformer.encoder.block.0.1.attention.self.value.weight\", \"segformer.segformer.encoder.block.0.1.attention.self.value.bias\", \"segformer.segformer.encoder.block.0.1.attention.self.sr.weight\", \"segformer.segformer.encoder.block.0.1.attention.self.sr.bias\", \"segformer.segformer.encoder.block.0.1.attention.self.layer_norm.weight\", \"segformer.segformer.encoder.block.0.1.attention.self.layer_norm.bias\", \"segformer.segformer.encoder.block.0.1.attention.output.dense.weight\", \"segformer.segformer.encoder.block.0.1.attention.output.dense.bias\", \"segformer.segformer.encoder.block.0.1.layer_norm_2.weight\", \"segformer.segformer.encoder.block.0.1.layer_norm_2.bias\", \"segformer.segformer.encoder.block.0.1.mlp.dense1.weight\", \"segformer.segformer.encoder.block.0.1.mlp.dense1.bias\", \"segformer.segformer.encoder.block.0.1.mlp.dwconv.dwconv.weight\", \"segformer.segformer.encoder.block.0.1.mlp.dwconv.dwconv.bias\", \"segformer.segformer.encoder.block.0.1.mlp.dense2.weight\", \"segformer.segformer.encoder.block.0.1.mlp.dense2.bias\", \"segformer.segformer.encoder.block.1.0.layer_norm_1.weight\", \"segformer.segformer.encoder.block.1.0.layer_norm_1.bias\", \"segformer.segformer.encoder.block.1.0.attention.self.query.weight\", \"segformer.segformer.encoder.block.1.0.attention.self.query.bias\", \"segformer.segformer.encoder.block.1.0.attention.self.key.weight\", \"segformer.segformer.encoder.block.1.0.attention.self.key.bias\", \"segformer.segformer.encoder.block.1.0.attention.self.value.weight\", \"segformer.segformer.encoder.block.1.0.attention.self.value.bias\", \"segformer.segformer.encoder.block.1.0.attention.self.sr.weight\", \"segformer.segformer.encoder.block.1.0.attention.self.sr.bias\", \"segformer.segformer.encoder.block.1.0.attention.self.layer_norm.weight\", \"segformer.segformer.encoder.block.1.0.attention.self.layer_norm.bias\", \"segformer.segformer.encoder.block.1.0.attention.output.dense.weight\", \"segformer.segformer.encoder.block.1.0.attention.output.dense.bias\", \"segformer.segformer.encoder.block.1.0.layer_norm_2.weight\", \"segformer.segformer.encoder.block.1.0.layer_norm_2.bias\", \"segformer.segformer.encoder.block.1.0.mlp.dense1.weight\", \"segformer.segformer.encoder.block.1.0.mlp.dense1.bias\", \"segformer.segformer.encoder.block.1.0.mlp.dwconv.dwconv.weight\", \"segformer.segformer.encoder.block.1.0.mlp.dwconv.dwconv.bias\", \"segformer.segformer.encoder.block.1.0.mlp.dense2.weight\", \"segformer.segformer.encoder.block.1.0.mlp.dense2.bias\", \"segformer.segformer.encoder.block.1.1.layer_norm_1.weight\", \"segformer.segformer.encoder.block.1.1.layer_norm_1.bias\", \"segformer.segformer.encoder.block.1.1.attention.self.query.weight\", \"segformer.segformer.encoder.block.1.1.attention.self.query.bias\", \"segformer.segformer.encoder.block.1.1.attention.self.key.weight\", \"segformer.segformer.encoder.block.1.1.attention.self.key.bias\", \"segformer.segformer.encoder.block.1.1.attention.self.value.weight\", \"segformer.segformer.encoder.block.1.1.attention.self.value.bias\", \"segformer.segformer.encoder.block.1.1.attention.self.sr.weight\", \"segformer.segformer.encoder.block.1.1.attention.self.sr.bias\", \"segformer.segformer.encoder.block.1.1.attention.self.layer_norm.weight\", \"segformer.segformer.encoder.block.1.1.attention.self.layer_norm.bias\", \"segformer.segformer.encoder.block.1.1.attention.output.dense.weight\", \"segformer.segformer.encoder.block.1.1.attention.output.dense.bias\", \"segformer.segformer.encoder.block.1.1.layer_norm_2.weight\", \"segformer.segformer.encoder.block.1.1.layer_norm_2.bias\", \"segformer.segformer.encoder.block.1.1.mlp.dense1.weight\", \"segformer.segformer.encoder.block.1.1.mlp.dense1.bias\", \"segformer.segformer.encoder.block.1.1.mlp.dwconv.dwconv.weight\", \"segformer.segformer.encoder.block.1.1.mlp.dwconv.dwconv.bias\", \"segformer.segformer.encoder.block.1.1.mlp.dense2.weight\", \"segformer.segformer.encoder.block.1.1.mlp.dense2.bias\", \"segformer.segformer.encoder.block.2.0.layer_norm_1.weight\", \"segformer.segformer.encoder.block.2.0.layer_norm_1.bias\", \"segformer.segformer.encoder.block.2.0.attention.self.query.weight\", \"segformer.segformer.encoder.block.2.0.attention.self.query.bias\", \"segformer.segformer.encoder.block.2.0.attention.self.key.weight\", \"segformer.segformer.encoder.block.2.0.attention.self.key.bias\", \"segformer.segformer.encoder.block.2.0.attention.self.value.weight\", \"segformer.segformer.encoder.block.2.0.attention.self.value.bias\", \"segformer.segformer.encoder.block.2.0.attention.self.sr.weight\", \"segformer.segformer.encoder.block.2.0.attention.self.sr.bias\", \"segformer.segformer.encoder.block.2.0.attention.self.layer_norm.weight\", \"segformer.segformer.encoder.block.2.0.attention.self.layer_norm.bias\", \"segformer.segformer.encoder.block.2.0.attention.output.dense.weight\", \"segformer.segformer.encoder.block.2.0.attention.output.dense.bias\", \"segformer.segformer.encoder.block.2.0.layer_norm_2.weight\", \"segformer.segformer.encoder.block.2.0.layer_norm_2.bias\", \"segformer.segformer.encoder.block.2.0.mlp.dense1.weight\", \"segformer.segformer.encoder.block.2.0.mlp.dense1.bias\", \"segformer.segformer.encoder.block.2.0.mlp.dwconv.dwconv.weight\", \"segformer.segformer.encoder.block.2.0.mlp.dwconv.dwconv.bias\", \"segformer.segformer.encoder.block.2.0.mlp.dense2.weight\", \"segformer.segformer.encoder.block.2.0.mlp.dense2.bias\", \"segformer.segformer.encoder.block.2.1.layer_norm_1.weight\", \"segformer.segformer.encoder.block.2.1.layer_norm_1.bias\", \"segformer.segformer.encoder.block.2.1.attention.self.query.weight\", \"segformer.segformer.encoder.block.2.1.attention.self.query.bias\", \"segformer.segformer.encoder.block.2.1.attention.self.key.weight\", \"segformer.segformer.encoder.block.2.1.attention.self.key.bias\", \"segformer.segformer.encoder.block.2.1.attention.self.value.weight\", \"segformer.segformer.encoder.block.2.1.attention.self.value.bias\", \"segformer.segformer.encoder.block.2.1.attention.self.sr.weight\", \"segformer.segformer.encoder.block.2.1.attention.self.sr.bias\", \"segformer.segformer.encoder.block.2.1.attention.self.layer_norm.weight\", \"segformer.segformer.encoder.block.2.1.attention.self.layer_norm.bias\", \"segformer.segformer.encoder.block.2.1.attention.output.dense.weight\", \"segformer.segformer.encoder.block.2.1.attention.output.dense.bias\", \"segformer.segformer.encoder.block.2.1.layer_norm_2.weight\", \"segformer.segformer.encoder.block.2.1.layer_norm_2.bias\", \"segformer.segformer.encoder.block.2.1.mlp.dense1.weight\", \"segformer.segformer.encoder.block.2.1.mlp.dense1.bias\", \"segformer.segformer.encoder.block.2.1.mlp.dwconv.dwconv.weight\", \"segformer.segformer.encoder.block.2.1.mlp.dwconv.dwconv.bias\", \"segformer.segformer.encoder.block.2.1.mlp.dense2.weight\", \"segformer.segformer.encoder.block.2.1.mlp.dense2.bias\", \"segformer.segformer.encoder.block.3.0.layer_norm_1.weight\", \"segformer.segformer.encoder.block.3.0.layer_norm_1.bias\", \"segformer.segformer.encoder.block.3.0.attention.self.query.weight\", \"segformer.segformer.encoder.block.3.0.attention.self.query.bias\", \"segformer.segformer.encoder.block.3.0.attention.self.key.weight\", \"segformer.segformer.encoder.block.3.0.attention.self.key.bias\", \"segformer.segformer.encoder.block.3.0.attention.self.value.weight\", \"segformer.segformer.encoder.block.3.0.attention.self.value.bias\", \"segformer.segformer.encoder.block.3.0.attention.output.dense.weight\", \"segformer.segformer.encoder.block.3.0.attention.output.dense.bias\", \"segformer.segformer.encoder.block.3.0.layer_norm_2.weight\", \"segformer.segformer.encoder.block.3.0.layer_norm_2.bias\", \"segformer.segformer.encoder.block.3.0.mlp.dense1.weight\", \"segformer.segformer.encoder.block.3.0.mlp.dense1.bias\", \"segformer.segformer.encoder.block.3.0.mlp.dwconv.dwconv.weight\", \"segformer.segformer.encoder.block.3.0.mlp.dwconv.dwconv.bias\", \"segformer.segformer.encoder.block.3.0.mlp.dense2.weight\", \"segformer.segformer.encoder.block.3.0.mlp.dense2.bias\", \"segformer.segformer.encoder.block.3.1.layer_norm_1.weight\", \"segformer.segformer.encoder.block.3.1.layer_norm_1.bias\", \"segformer.segformer.encoder.block.3.1.attention.self.query.weight\", \"segformer.segformer.encoder.block.3.1.attention.self.query.bias\", \"segformer.segformer.encoder.block.3.1.attention.self.key.weight\", \"segformer.segformer.encoder.block.3.1.attention.self.key.bias\", \"segformer.segformer.encoder.block.3.1.attention.self.value.weight\", \"segformer.segformer.encoder.block.3.1.attention.self.value.bias\", \"segformer.segformer.encoder.block.3.1.attention.output.dense.weight\", \"segformer.segformer.encoder.block.3.1.attention.output.dense.bias\", \"segformer.segformer.encoder.block.3.1.layer_norm_2.weight\", \"segformer.segformer.encoder.block.3.1.layer_norm_2.bias\", \"segformer.segformer.encoder.block.3.1.mlp.dense1.weight\", \"segformer.segformer.encoder.block.3.1.mlp.dense1.bias\", \"segformer.segformer.encoder.block.3.1.mlp.dwconv.dwconv.weight\", \"segformer.segformer.encoder.block.3.1.mlp.dwconv.dwconv.bias\", \"segformer.segformer.encoder.block.3.1.mlp.dense2.weight\", \"segformer.segformer.encoder.block.3.1.mlp.dense2.bias\", \"segformer.segformer.encoder.layer_norm.0.weight\", \"segformer.segformer.encoder.layer_norm.0.bias\", \"segformer.segformer.encoder.layer_norm.1.weight\", \"segformer.segformer.encoder.layer_norm.1.bias\", \"segformer.segformer.encoder.layer_norm.2.weight\", \"segformer.segformer.encoder.layer_norm.2.bias\", \"segformer.segformer.encoder.layer_norm.3.weight\", \"segformer.segformer.encoder.layer_norm.3.bias\", \"segformer.decode_head.linear_c.0.proj.weight\", \"segformer.decode_head.linear_c.0.proj.bias\", \"segformer.decode_head.linear_c.1.proj.weight\", \"segformer.decode_head.linear_c.1.proj.bias\", \"segformer.decode_head.linear_c.2.proj.weight\", \"segformer.decode_head.linear_c.2.proj.bias\", \"segformer.decode_head.linear_c.3.proj.weight\", \"segformer.decode_head.linear_c.3.proj.bias\", \"segformer.decode_head.linear_fuse.weight\", \"segformer.decode_head.batch_norm.weight\", \"segformer.decode_head.batch_norm.bias\", \"segformer.decode_head.batch_norm.running_mean\", \"segformer.decode_head.batch_norm.running_var\", \"segformer.decode_head.batch_norm.num_batches_tracked\", \"segformer.decode_head.classifier.weight\", \"segformer.decode_head.classifier.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     10\u001b[39m segformer_model = SegformerForSemanticSegmentation.from_pretrained(\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnvidia/segformer-b0-finetuned-ade-512-512\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     num_labels=\u001b[32m1\u001b[39m,\n\u001b[32m     13\u001b[39m     ignore_mismatched_sizes=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     14\u001b[39m ).to(device)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Load state dict\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43msegformer_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodels/segformer_model_best.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m segformer_model.eval()\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m✅ SegFormer model loaded successfully!\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2624\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2616\u001b[39m         error_msgs.insert(\n\u001b[32m   2617\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2618\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2619\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2620\u001b[39m             ),\n\u001b[32m   2621\u001b[39m         )\n\u001b[32m   2623\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2624\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2625\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2626\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2627\u001b[39m         )\n\u001b[32m   2628\u001b[39m     )\n\u001b[32m   2629\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for SegformerForSemanticSegmentation:\n\tMissing key(s) in state_dict: \"segformer.encoder.patch_embeddings.0.proj.weight\", \"segformer.encoder.patch_embeddings.0.proj.bias\", \"segformer.encoder.patch_embeddings.0.layer_norm.weight\", \"segformer.encoder.patch_embeddings.0.layer_norm.bias\", \"segformer.encoder.patch_embeddings.1.proj.weight\", \"segformer.encoder.patch_embeddings.1.proj.bias\", \"segformer.encoder.patch_embeddings.1.layer_norm.weight\", \"segformer.encoder.patch_embeddings.1.layer_norm.bias\", \"segformer.encoder.patch_embeddings.2.proj.weight\", \"segformer.encoder.patch_embeddings.2.proj.bias\", \"segformer.encoder.patch_embeddings.2.layer_norm.weight\", \"segformer.encoder.patch_embeddings.2.layer_norm.bias\", \"segformer.encoder.patch_embeddings.3.proj.weight\", \"segformer.encoder.patch_embeddings.3.proj.bias\", \"segformer.encoder.patch_embeddings.3.layer_norm.weight\", \"segformer.encoder.patch_embeddings.3.layer_norm.bias\", \"segformer.encoder.block.0.0.layer_norm_1.weight\", \"segformer.encoder.block.0.0.layer_norm_1.bias\", \"segformer.encoder.block.0.0.attention.self.query.weight\", \"segformer.encoder.block.0.0.attention.self.query.bias\", \"segformer.encoder.block.0.0.attention.self.key.weight\", \"segformer.encoder.block.0.0.attention.self.key.bias\", \"segformer.encoder.block.0.0.attention.self.value.weight\", \"segformer.encoder.block.0.0.attention.self.value.bias\", \"segformer.encoder.block.0.0.attention.self.sr.weight\", \"segformer.encoder.block.0.0.attention.self.sr.bias\", \"segformer.encoder.block.0.0.attention.self.layer_norm.weight\", \"segformer.encoder.block.0.0.attention.self.layer_norm.bias\", \"segformer.encoder.block.0.0.attention.output.dense.weight\", \"segformer.encoder.block.0.0.attention.output.dense.bias\", \"segformer.encoder.block.0.0.layer_norm_2.weight\", \"segformer.encoder.block.0.0.layer_norm_2.bias\", \"segformer.encoder.block.0.0.mlp.dense1.weight\", \"segformer.encoder.block.0.0.mlp.dense1.bias\", \"segformer.encoder.block.0.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.0.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.0.0.mlp.dense2.weight\", \"segformer.encoder.block.0.0.mlp.dense2.bias\", \"segformer.encoder.block.0.1.layer_norm_1.weight\", \"segformer.encoder.block.0.1.layer_norm_1.bias\", \"segformer.encoder.block.0.1.attention.self.query.weight\", \"segformer.encoder.block.0.1.attention.self.query.bias\", \"segformer.encoder.block.0.1.attention.self.key.weight\", \"segformer.encoder.block.0.1.attention.self.key.bias\", \"segformer.encoder.block.0.1.attention.self.value.weight\", \"segformer.encoder.block.0.1.attention.self.value.bias\", \"segformer.encoder.block.0.1.attention.self.sr.weight\", \"segformer.encoder.block.0.1.attention.self.sr.bias\", \"segformer.encoder.block.0.1.attention.self.layer_norm.weight\", \"segformer.encoder.block.0.1.attention.self.layer_norm.bias\", \"segformer.encoder.block.0.1.attention.output.dense.weight\", \"segformer.encoder.block.0.1.attention.output.dense.bias\", \"segformer.encoder.block.0.1.layer_norm_2.weight\", \"segformer.encoder.block.0.1.layer_norm_2.bias\", \"segformer.encoder.block.0.1.mlp.dense1.weight\", \"segformer.encoder.block.0.1.mlp.dense1.bias\", \"segformer.encoder.block.0.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.0.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.0.1.mlp.dense2.weight\", \"segformer.encoder.block.0.1.mlp.dense2.bias\", \"segformer.encoder.block.1.0.layer_norm_1.weight\", \"segformer.encoder.block.1.0.layer_norm_1.bias\", \"segformer.encoder.block.1.0.attention.self.query.weight\", \"segformer.encoder.block.1.0.attention.self.query.bias\", \"segformer.encoder.block.1.0.attention.self.key.weight\", \"segformer.encoder.block.1.0.attention.self.key.bias\", \"segformer.encoder.block.1.0.attention.self.value.weight\", \"segformer.encoder.block.1.0.attention.self.value.bias\", \"segformer.encoder.block.1.0.attention.self.sr.weight\", \"segformer.encoder.block.1.0.attention.self.sr.bias\", \"segformer.encoder.block.1.0.attention.self.layer_norm.weight\", \"segformer.encoder.block.1.0.attention.self.layer_norm.bias\", \"segformer.encoder.block.1.0.attention.output.dense.weight\", \"segformer.encoder.block.1.0.attention.output.dense.bias\", \"segformer.encoder.block.1.0.layer_norm_2.weight\", \"segformer.encoder.block.1.0.layer_norm_2.bias\", \"segformer.encoder.block.1.0.mlp.dense1.weight\", \"segformer.encoder.block.1.0.mlp.dense1.bias\", \"segformer.encoder.block.1.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.1.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.1.0.mlp.dense2.weight\", \"segformer.encoder.block.1.0.mlp.dense2.bias\", \"segformer.encoder.block.1.1.layer_norm_1.weight\", \"segformer.encoder.block.1.1.layer_norm_1.bias\", \"segformer.encoder.block.1.1.attention.self.query.weight\", \"segformer.encoder.block.1.1.attention.self.query.bias\", \"segformer.encoder.block.1.1.attention.self.key.weight\", \"segformer.encoder.block.1.1.attention.self.key.bias\", \"segformer.encoder.block.1.1.attention.self.value.weight\", \"segformer.encoder.block.1.1.attention.self.value.bias\", \"segformer.encoder.block.1.1.attention.self.sr.weight\", \"segformer.encoder.block.1.1.attention.self.sr.bias\", \"segformer.encoder.block.1.1.attention.self.layer_norm.weight\", \"segformer.encoder.block.1.1.attention.self.layer_norm.bias\", \"segformer.encoder.block.1.1.attention.output.dense.weight\", \"segformer.encoder.block.1.1.attention.output.dense.bias\", \"segformer.encoder.block.1.1.layer_norm_2.weight\", \"segformer.encoder.block.1.1.layer_norm_2.bias\", \"segformer.encoder.block.1.1.mlp.dense1.weight\", \"segformer.encoder.block.1.1.mlp.dense1.bias\", \"segformer.encoder.block.1.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.1.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.1.1.mlp.dense2.weight\", \"segformer.encoder.block.1.1.mlp.dense2.bias\", \"segformer.encoder.block.2.0.layer_norm_1.weight\", \"segformer.encoder.block.2.0.layer_norm_1.bias\", \"segformer.encoder.block.2.0.attention.self.query.weight\", \"segformer.encoder.block.2.0.attention.self.query.bias\", \"segformer.encoder.block.2.0.attention.self.key.weight\", \"segformer.encoder.block.2.0.attention.self.key.bias\", \"segformer.encoder.block.2.0.attention.self.value.weight\", \"segformer.encoder.block.2.0.attention.self.value.bias\", \"segformer.encoder.block.2.0.attention.self.sr.weight\", \"segformer.encoder.block.2.0.attention.self.sr.bias\", \"segformer.encoder.block.2.0.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.0.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.0.attention.output.dense.weight\", \"segformer.encoder.block.2.0.attention.output.dense.bias\", \"segformer.encoder.block.2.0.layer_norm_2.weight\", \"segformer.encoder.block.2.0.layer_norm_2.bias\", \"segformer.encoder.block.2.0.mlp.dense1.weight\", \"segformer.encoder.block.2.0.mlp.dense1.bias\", \"segformer.encoder.block.2.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.0.mlp.dense2.weight\", \"segformer.encoder.block.2.0.mlp.dense2.bias\", \"segformer.encoder.block.2.1.layer_norm_1.weight\", \"segformer.encoder.block.2.1.layer_norm_1.bias\", \"segformer.encoder.block.2.1.attention.self.query.weight\", \"segformer.encoder.block.2.1.attention.self.query.bias\", \"segformer.encoder.block.2.1.attention.self.key.weight\", \"segformer.encoder.block.2.1.attention.self.key.bias\", \"segformer.encoder.block.2.1.attention.self.value.weight\", \"segformer.encoder.block.2.1.attention.self.value.bias\", \"segformer.encoder.block.2.1.attention.self.sr.weight\", \"segformer.encoder.block.2.1.attention.self.sr.bias\", \"segformer.encoder.block.2.1.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.1.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.1.attention.output.dense.weight\", \"segformer.encoder.block.2.1.attention.output.dense.bias\", \"segformer.encoder.block.2.1.layer_norm_2.weight\", \"segformer.encoder.block.2.1.layer_norm_2.bias\", \"segformer.encoder.block.2.1.mlp.dense1.weight\", \"segformer.encoder.block.2.1.mlp.dense1.bias\", \"segformer.encoder.block.2.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.1.mlp.dense2.weight\", \"segformer.encoder.block.2.1.mlp.dense2.bias\", \"segformer.encoder.block.3.0.layer_norm_1.weight\", \"segformer.encoder.block.3.0.layer_norm_1.bias\", \"segformer.encoder.block.3.0.attention.self.query.weight\", \"segformer.encoder.block.3.0.attention.self.query.bias\", \"segformer.encoder.block.3.0.attention.self.key.weight\", \"segformer.encoder.block.3.0.attention.self.key.bias\", \"segformer.encoder.block.3.0.attention.self.value.weight\", \"segformer.encoder.block.3.0.attention.self.value.bias\", \"segformer.encoder.block.3.0.attention.output.dense.weight\", \"segformer.encoder.block.3.0.attention.output.dense.bias\", \"segformer.encoder.block.3.0.layer_norm_2.weight\", \"segformer.encoder.block.3.0.layer_norm_2.bias\", \"segformer.encoder.block.3.0.mlp.dense1.weight\", \"segformer.encoder.block.3.0.mlp.dense1.bias\", \"segformer.encoder.block.3.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.3.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.3.0.mlp.dense2.weight\", \"segformer.encoder.block.3.0.mlp.dense2.bias\", \"segformer.encoder.block.3.1.layer_norm_1.weight\", \"segformer.encoder.block.3.1.layer_norm_1.bias\", \"segformer.encoder.block.3.1.attention.self.query.weight\", \"segformer.encoder.block.3.1.attention.self.query.bias\", \"segformer.encoder.block.3.1.attention.self.key.weight\", \"segformer.encoder.block.3.1.attention.self.key.bias\", \"segformer.encoder.block.3.1.attention.self.value.weight\", \"segformer.encoder.block.3.1.attention.self.value.bias\", \"segformer.encoder.block.3.1.attention.output.dense.weight\", \"segformer.encoder.block.3.1.attention.output.dense.bias\", \"segformer.encoder.block.3.1.layer_norm_2.weight\", \"segformer.encoder.block.3.1.layer_norm_2.bias\", \"segformer.encoder.block.3.1.mlp.dense1.weight\", \"segformer.encoder.block.3.1.mlp.dense1.bias\", \"segformer.encoder.block.3.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.3.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.3.1.mlp.dense2.weight\", \"segformer.encoder.block.3.1.mlp.dense2.bias\", \"segformer.encoder.layer_norm.0.weight\", \"segformer.encoder.layer_norm.0.bias\", \"segformer.encoder.layer_norm.1.weight\", \"segformer.encoder.layer_norm.1.bias\", \"segformer.encoder.layer_norm.2.weight\", \"segformer.encoder.layer_norm.2.bias\", \"segformer.encoder.layer_norm.3.weight\", \"segformer.encoder.layer_norm.3.bias\", \"decode_head.linear_c.0.proj.weight\", \"decode_head.linear_c.0.proj.bias\", \"decode_head.linear_c.1.proj.weight\", \"decode_head.linear_c.1.proj.bias\", \"decode_head.linear_c.2.proj.weight\", \"decode_head.linear_c.2.proj.bias\", \"decode_head.linear_c.3.proj.weight\", \"decode_head.linear_c.3.proj.bias\", \"decode_head.linear_fuse.weight\", \"decode_head.batch_norm.weight\", \"decode_head.batch_norm.bias\", \"decode_head.batch_norm.running_mean\", \"decode_head.batch_norm.running_var\", \"decode_head.classifier.weight\", \"decode_head.classifier.bias\". \n\tUnexpected key(s) in state_dict: \"segformer.segformer.encoder.patch_embeddings.0.proj.weight\", \"segformer.segformer.encoder.patch_embeddings.0.proj.bias\", \"segformer.segformer.encoder.patch_embeddings.0.layer_norm.weight\", \"segformer.segformer.encoder.patch_embeddings.0.layer_norm.bias\", \"segformer.segformer.encoder.patch_embeddings.1.proj.weight\", \"segformer.segformer.encoder.patch_embeddings.1.proj.bias\", \"segformer.segformer.encoder.patch_embeddings.1.layer_norm.weight\", \"segformer.segformer.encoder.patch_embeddings.1.layer_norm.bias\", \"segformer.segformer.encoder.patch_embeddings.2.proj.weight\", \"segformer.segformer.encoder.patch_embeddings.2.proj.bias\", \"segformer.segformer.encoder.patch_embeddings.2.layer_norm.weight\", \"segformer.segformer.encoder.patch_embeddings.2.layer_norm.bias\", \"segformer.segformer.encoder.patch_embeddings.3.proj.weight\", \"segformer.segformer.encoder.patch_embeddings.3.proj.bias\", \"segformer.segformer.encoder.patch_embeddings.3.layer_norm.weight\", \"segformer.segformer.encoder.patch_embeddings.3.layer_norm.bias\", \"segformer.segformer.encoder.block.0.0.layer_norm_1.weight\", \"segformer.segformer.encoder.block.0.0.layer_norm_1.bias\", \"segformer.segformer.encoder.block.0.0.attention.self.query.weight\", \"segformer.segformer.encoder.block.0.0.attention.self.query.bias\", \"segformer.segformer.encoder.block.0.0.attention.self.key.weight\", \"segformer.segformer.encoder.block.0.0.attention.self.key.bias\", \"segformer.segformer.encoder.block.0.0.attention.self.value.weight\", \"segformer.segformer.encoder.block.0.0.attention.self.value.bias\", \"segformer.segformer.encoder.block.0.0.attention.self.sr.weight\", \"segformer.segformer.encoder.block.0.0.attention.self.sr.bias\", \"segformer.segformer.encoder.block.0.0.attention.self.layer_norm.weight\", \"segformer.segformer.encoder.block.0.0.attention.self.layer_norm.bias\", \"segformer.segformer.encoder.block.0.0.attention.output.dense.weight\", \"segformer.segformer.encoder.block.0.0.attention.output.dense.bias\", \"segformer.segformer.encoder.block.0.0.layer_norm_2.weight\", \"segformer.segformer.encoder.block.0.0.layer_norm_2.bias\", \"segformer.segformer.encoder.block.0.0.mlp.dense1.weight\", \"segformer.segformer.encoder.block.0.0.mlp.dense1.bias\", \"segformer.segformer.encoder.block.0.0.mlp.dwconv.dwconv.weight\", \"segformer.segformer.encoder.block.0.0.mlp.dwconv.dwconv.bias\", \"segformer.segformer.encoder.block.0.0.mlp.dense2.weight\", \"segformer.segformer.encoder.block.0.0.mlp.dense2.bias\", \"segformer.segformer.encoder.block.0.1.layer_norm_1.weight\", \"segformer.segformer.encoder.block.0.1.layer_norm_1.bias\", \"segformer.segformer.encoder.block.0.1.attention.self.query.weight\", \"segformer.segformer.encoder.block.0.1.attention.self.query.bias\", \"segformer.segformer.encoder.block.0.1.attention.self.key.weight\", \"segformer.segformer.encoder.block.0.1.attention.self.key.bias\", \"segformer.segformer.encoder.block.0.1.attention.self.value.weight\", \"segformer.segformer.encoder.block.0.1.attention.self.value.bias\", \"segformer.segformer.encoder.block.0.1.attention.self.sr.weight\", \"segformer.segformer.encoder.block.0.1.attention.self.sr.bias\", \"segformer.segformer.encoder.block.0.1.attention.self.layer_norm.weight\", \"segformer.segformer.encoder.block.0.1.attention.self.layer_norm.bias\", \"segformer.segformer.encoder.block.0.1.attention.output.dense.weight\", \"segformer.segformer.encoder.block.0.1.attention.output.dense.bias\", \"segformer.segformer.encoder.block.0.1.layer_norm_2.weight\", \"segformer.segformer.encoder.block.0.1.layer_norm_2.bias\", \"segformer.segformer.encoder.block.0.1.mlp.dense1.weight\", \"segformer.segformer.encoder.block.0.1.mlp.dense1.bias\", \"segformer.segformer.encoder.block.0.1.mlp.dwconv.dwconv.weight\", \"segformer.segformer.encoder.block.0.1.mlp.dwconv.dwconv.bias\", \"segformer.segformer.encoder.block.0.1.mlp.dense2.weight\", \"segformer.segformer.encoder.block.0.1.mlp.dense2.bias\", \"segformer.segformer.encoder.block.1.0.layer_norm_1.weight\", \"segformer.segformer.encoder.block.1.0.layer_norm_1.bias\", \"segformer.segformer.encoder.block.1.0.attention.self.query.weight\", \"segformer.segformer.encoder.block.1.0.attention.self.query.bias\", \"segformer.segformer.encoder.block.1.0.attention.self.key.weight\", \"segformer.segformer.encoder.block.1.0.attention.self.key.bias\", \"segformer.segformer.encoder.block.1.0.attention.self.value.weight\", \"segformer.segformer.encoder.block.1.0.attention.self.value.bias\", \"segformer.segformer.encoder.block.1.0.attention.self.sr.weight\", \"segformer.segformer.encoder.block.1.0.attention.self.sr.bias\", \"segformer.segformer.encoder.block.1.0.attention.self.layer_norm.weight\", \"segformer.segformer.encoder.block.1.0.attention.self.layer_norm.bias\", \"segformer.segformer.encoder.block.1.0.attention.output.dense.weight\", \"segformer.segformer.encoder.block.1.0.attention.output.dense.bias\", \"segformer.segformer.encoder.block.1.0.layer_norm_2.weight\", \"segformer.segformer.encoder.block.1.0.layer_norm_2.bias\", \"segformer.segformer.encoder.block.1.0.mlp.dense1.weight\", \"segformer.segformer.encoder.block.1.0.mlp.dense1.bias\", \"segformer.segformer.encoder.block.1.0.mlp.dwconv.dwconv.weight\", \"segformer.segformer.encoder.block.1.0.mlp.dwconv.dwconv.bias\", \"segformer.segformer.encoder.block.1.0.mlp.dense2.weight\", \"segformer.segformer.encoder.block.1.0.mlp.dense2.bias\", \"segformer.segformer.encoder.block.1.1.layer_norm_1.weight\", \"segformer.segformer.encoder.block.1.1.layer_norm_1.bias\", \"segformer.segformer.encoder.block.1.1.attention.self.query.weight\", \"segformer.segformer.encoder.block.1.1.attention.self.query.bias\", \"segformer.segformer.encoder.block.1.1.attention.self.key.weight\", \"segformer.segformer.encoder.block.1.1.attention.self.key.bias\", \"segformer.segformer.encoder.block.1.1.attention.self.value.weight\", \"segformer.segformer.encoder.block.1.1.attention.self.value.bias\", \"segformer.segformer.encoder.block.1.1.attention.self.sr.weight\", \"segformer.segformer.encoder.block.1.1.attention.self.sr.bias\", \"segformer.segformer.encoder.block.1.1.attention.self.layer_norm.weight\", \"segformer.segformer.encoder.block.1.1.attention.self.layer_norm.bias\", \"segformer.segformer.encoder.block.1.1.attention.output.dense.weight\", \"segformer.segformer.encoder.block.1.1.attention.output.dense.bias\", \"segformer.segformer.encoder.block.1.1.layer_norm_2.weight\", \"segformer.segformer.encoder.block.1.1.layer_norm_2.bias\", \"segformer.segformer.encoder.block.1.1.mlp.dense1.weight\", \"segformer.segformer.encoder.block.1.1.mlp.dense1.bias\", \"segformer.segformer.encoder.block.1.1.mlp.dwconv.dwconv.weight\", \"segformer.segformer.encoder.block.1.1.mlp.dwconv.dwconv.bias\", \"segformer.segformer.encoder.block.1.1.mlp.dense2.weight\", \"segformer.segformer.encoder.block.1.1.mlp.dense2.bias\", \"segformer.segformer.encoder.block.2.0.layer_norm_1.weight\", \"segformer.segformer.encoder.block.2.0.layer_norm_1.bias\", \"segformer.segformer.encoder.block.2.0.attention.self.query.weight\", \"segformer.segformer.encoder.block.2.0.attention.self.query.bias\", \"segformer.segformer.encoder.block.2.0.attention.self.key.weight\", \"segformer.segformer.encoder.block.2.0.attention.self.key.bias\", \"segformer.segformer.encoder.block.2.0.attention.self.value.weight\", \"segformer.segformer.encoder.block.2.0.attention.self.value.bias\", \"segformer.segformer.encoder.block.2.0.attention.self.sr.weight\", \"segformer.segformer.encoder.block.2.0.attention.self.sr.bias\", \"segformer.segformer.encoder.block.2.0.attention.self.layer_norm.weight\", \"segformer.segformer.encoder.block.2.0.attention.self.layer_norm.bias\", \"segformer.segformer.encoder.block.2.0.attention.output.dense.weight\", \"segformer.segformer.encoder.block.2.0.attention.output.dense.bias\", \"segformer.segformer.encoder.block.2.0.layer_norm_2.weight\", \"segformer.segformer.encoder.block.2.0.layer_norm_2.bias\", \"segformer.segformer.encoder.block.2.0.mlp.dense1.weight\", \"segformer.segformer.encoder.block.2.0.mlp.dense1.bias\", \"segformer.segformer.encoder.block.2.0.mlp.dwconv.dwconv.weight\", \"segformer.segformer.encoder.block.2.0.mlp.dwconv.dwconv.bias\", \"segformer.segformer.encoder.block.2.0.mlp.dense2.weight\", \"segformer.segformer.encoder.block.2.0.mlp.dense2.bias\", \"segformer.segformer.encoder.block.2.1.layer_norm_1.weight\", \"segformer.segformer.encoder.block.2.1.layer_norm_1.bias\", \"segformer.segformer.encoder.block.2.1.attention.self.query.weight\", \"segformer.segformer.encoder.block.2.1.attention.self.query.bias\", \"segformer.segformer.encoder.block.2.1.attention.self.key.weight\", \"segformer.segformer.encoder.block.2.1.attention.self.key.bias\", \"segformer.segformer.encoder.block.2.1.attention.self.value.weight\", \"segformer.segformer.encoder.block.2.1.attention.self.value.bias\", \"segformer.segformer.encoder.block.2.1.attention.self.sr.weight\", \"segformer.segformer.encoder.block.2.1.attention.self.sr.bias\", \"segformer.segformer.encoder.block.2.1.attention.self.layer_norm.weight\", \"segformer.segformer.encoder.block.2.1.attention.self.layer_norm.bias\", \"segformer.segformer.encoder.block.2.1.attention.output.dense.weight\", \"segformer.segformer.encoder.block.2.1.attention.output.dense.bias\", \"segformer.segformer.encoder.block.2.1.layer_norm_2.weight\", \"segformer.segformer.encoder.block.2.1.layer_norm_2.bias\", \"segformer.segformer.encoder.block.2.1.mlp.dense1.weight\", \"segformer.segformer.encoder.block.2.1.mlp.dense1.bias\", \"segformer.segformer.encoder.block.2.1.mlp.dwconv.dwconv.weight\", \"segformer.segformer.encoder.block.2.1.mlp.dwconv.dwconv.bias\", \"segformer.segformer.encoder.block.2.1.mlp.dense2.weight\", \"segformer.segformer.encoder.block.2.1.mlp.dense2.bias\", \"segformer.segformer.encoder.block.3.0.layer_norm_1.weight\", \"segformer.segformer.encoder.block.3.0.layer_norm_1.bias\", \"segformer.segformer.encoder.block.3.0.attention.self.query.weight\", \"segformer.segformer.encoder.block.3.0.attention.self.query.bias\", \"segformer.segformer.encoder.block.3.0.attention.self.key.weight\", \"segformer.segformer.encoder.block.3.0.attention.self.key.bias\", \"segformer.segformer.encoder.block.3.0.attention.self.value.weight\", \"segformer.segformer.encoder.block.3.0.attention.self.value.bias\", \"segformer.segformer.encoder.block.3.0.attention.output.dense.weight\", \"segformer.segformer.encoder.block.3.0.attention.output.dense.bias\", \"segformer.segformer.encoder.block.3.0.layer_norm_2.weight\", \"segformer.segformer.encoder.block.3.0.layer_norm_2.bias\", \"segformer.segformer.encoder.block.3.0.mlp.dense1.weight\", \"segformer.segformer.encoder.block.3.0.mlp.dense1.bias\", \"segformer.segformer.encoder.block.3.0.mlp.dwconv.dwconv.weight\", \"segformer.segformer.encoder.block.3.0.mlp.dwconv.dwconv.bias\", \"segformer.segformer.encoder.block.3.0.mlp.dense2.weight\", \"segformer.segformer.encoder.block.3.0.mlp.dense2.bias\", \"segformer.segformer.encoder.block.3.1.layer_norm_1.weight\", \"segformer.segformer.encoder.block.3.1.layer_norm_1.bias\", \"segformer.segformer.encoder.block.3.1.attention.self.query.weight\", \"segformer.segformer.encoder.block.3.1.attention.self.query.bias\", \"segformer.segformer.encoder.block.3.1.attention.self.key.weight\", \"segformer.segformer.encoder.block.3.1.attention.self.key.bias\", \"segformer.segformer.encoder.block.3.1.attention.self.value.weight\", \"segformer.segformer.encoder.block.3.1.attention.self.value.bias\", \"segformer.segformer.encoder.block.3.1.attention.output.dense.weight\", \"segformer.segformer.encoder.block.3.1.attention.output.dense.bias\", \"segformer.segformer.encoder.block.3.1.layer_norm_2.weight\", \"segformer.segformer.encoder.block.3.1.layer_norm_2.bias\", \"segformer.segformer.encoder.block.3.1.mlp.dense1.weight\", \"segformer.segformer.encoder.block.3.1.mlp.dense1.bias\", \"segformer.segformer.encoder.block.3.1.mlp.dwconv.dwconv.weight\", \"segformer.segformer.encoder.block.3.1.mlp.dwconv.dwconv.bias\", \"segformer.segformer.encoder.block.3.1.mlp.dense2.weight\", \"segformer.segformer.encoder.block.3.1.mlp.dense2.bias\", \"segformer.segformer.encoder.layer_norm.0.weight\", \"segformer.segformer.encoder.layer_norm.0.bias\", \"segformer.segformer.encoder.layer_norm.1.weight\", \"segformer.segformer.encoder.layer_norm.1.bias\", \"segformer.segformer.encoder.layer_norm.2.weight\", \"segformer.segformer.encoder.layer_norm.2.bias\", \"segformer.segformer.encoder.layer_norm.3.weight\", \"segformer.segformer.encoder.layer_norm.3.bias\", \"segformer.decode_head.linear_c.0.proj.weight\", \"segformer.decode_head.linear_c.0.proj.bias\", \"segformer.decode_head.linear_c.1.proj.weight\", \"segformer.decode_head.linear_c.1.proj.bias\", \"segformer.decode_head.linear_c.2.proj.weight\", \"segformer.decode_head.linear_c.2.proj.bias\", \"segformer.decode_head.linear_c.3.proj.weight\", \"segformer.decode_head.linear_c.3.proj.bias\", \"segformer.decode_head.linear_fuse.weight\", \"segformer.decode_head.batch_norm.weight\", \"segformer.decode_head.batch_norm.bias\", \"segformer.decode_head.batch_norm.running_mean\", \"segformer.decode_head.batch_norm.running_var\", \"segformer.decode_head.batch_norm.num_batches_tracked\", \"segformer.decode_head.classifier.weight\", \"segformer.decode_head.classifier.bias\". "
     ]
    }
   ],
   "source": [
    "# SegFormer Model Evaluation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🔍 EVALUATING SEGFORMER MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if os.path.exists('models/segformer_model_best.pth'):\n",
    "    print('🔄 Loading SegFormer model...')\n",
    "    \n",
    "    # Load SegFormer (same as training)\n",
    "    segformer_model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "        \"nvidia/segformer-b0-finetuned-ade-512-512\",\n",
    "        num_labels=1,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load state dict with prefix handling\n",
    "    state_dict = torch.load('models/segformer_model_best.pth')\n",
    "    \n",
    "    # Handle potential prefix mismatch\n",
    "    if any(key.startswith('segformer.segformer.') for key in state_dict.keys()):\n",
    "        # Remove extra 'segformer.' prefix\n",
    "        new_state_dict = {}\n",
    "        for key, value in state_dict.items():\n",
    "            if key.startswith('segformer.segformer.'):\n",
    "                new_key = key.replace('segformer.segformer.', 'segformer.')\n",
    "                new_state_dict[new_key] = value\n",
    "            elif key.startswith('segformer.decode_head.'):\n",
    "                new_key = key.replace('segformer.decode_head.', 'decode_head.')\n",
    "                new_state_dict[new_key] = value\n",
    "            else:\n",
    "                new_state_dict[key] = value\n",
    "        state_dict = new_state_dict\n",
    "    \n",
    "    segformer_model.load_state_dict(state_dict)\n",
    "    segformer_model.eval()\n",
    "    \n",
    "    print('✅ SegFormer model loaded successfully!')\n",
    "    \n",
    "    # Evaluate\n",
    "    segformer_results = evaluate_model(segformer_model, test_loader, 'SegFormer')\n",
    "    \n",
    "    # Save results\n",
    "    with open('models/segformer_test_results.json', 'w') as f:\n",
    "        results_to_save = segformer_results.copy()\n",
    "        results_to_save['dice_scores'] = [float(x) for x in segformer_results['dice_scores']]\n",
    "        results_to_save['jaccard_scores'] = [float(x) for x in segformer_results['jaccard_scores']]\n",
    "        json.dump(results_to_save, f, indent=2)\n",
    "    \n",
    "    print('💾 SegFormer results saved!')\n",
    "else:\n",
    "    print('❌ SegFormer model not found. Please train it first.')\n",
    "    segformer_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Model 2: U-Net EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net EfficientNet Model Evaluation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🔍 EVALUATING U-NET EFFICIENTNET MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if os.path.exists('models/unet_efficientnet_model_best.pth'):\n",
    "    print('🔄 Loading U-Net EfficientNet model...')\n",
    "    \n",
    "    # Load U-Net EfficientNet (same as training)\n",
    "    unet_efficientnet_model = smp.Unet(\n",
    "        encoder_name=\"efficientnet-b0\",\n",
    "        encoder_weights=\"imagenet\",\n",
    "        in_channels=3,\n",
    "        classes=1,\n",
    "        activation=None  # We handle sigmoid manually\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load state dict\n",
    "    unet_efficientnet_model.load_state_dict(torch.load('models/unet_efficientnet_model_best.pth'))\n",
    "    unet_efficientnet_model.eval()\n",
    "    \n",
    "    print('✅ U-Net EfficientNet model loaded successfully!')\n",
    "    \n",
    "    # Evaluate\n",
    "    unet_efficientnet_results = evaluate_model(unet_efficientnet_model, test_loader, 'U-Net EfficientNet')\n",
    "    \n",
    "    # Save results\n",
    "    with open('models/unet_efficientnet_test_results.json', 'w') as f:\n",
    "        results_to_save = unet_efficientnet_results.copy()\n",
    "        results_to_save['dice_scores'] = [float(x) for x in unet_efficientnet_results['dice_scores']]\n",
    "        results_to_save['jaccard_scores'] = [float(x) for x in unet_efficientnet_results['jaccard_scores']]\n",
    "        json.dump(results_to_save, f, indent=2)\n",
    "    \n",
    "    print('💾 U-Net EfficientNet results saved!')\n",
    "else:\n",
    "    print('❌ U-Net EfficientNet model not found. Please train it first.')\n",
    "    unet_efficientnet_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Model 3: ViT Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT Segmentation Model Definition (same as training)\n",
    "class ViTSegmentation(nn.Module):\n",
    "    def __init__(self, vit_model_name=\"vit_tiny_patch16_224\", num_classes=1, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load Vision Transformer từ timm\n",
    "        self.vit = timm.create_model(\n",
    "            vit_model_name, \n",
    "            pretrained=pretrained,\n",
    "            num_classes=0  # Remove classification head\n",
    "        )\n",
    "        \n",
    "        # Get feature dimension\n",
    "        self.feature_dim = self.vit.num_features\n",
    "        \n",
    "        # Simple segmentation head\n",
    "        self.seg_head = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 14 * 14 * num_classes)  # 14x14 patches for 224x224 input\n",
    "        )\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Resize input cho ViT (224x224)\n",
    "        original_size = x.shape[-2:]\n",
    "        x_resized = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Extract global features từ ViT\n",
    "        features = self.vit(x_resized)  # [batch, feature_dim]\n",
    "        \n",
    "        # Generate segmentation map\n",
    "        seg_output = self.seg_head(features)  # [batch, 14*14*num_classes]\n",
    "        \n",
    "        # Reshape to spatial format\n",
    "        batch_size = seg_output.shape[0]\n",
    "        seg_map = seg_output.view(batch_size, self.num_classes, 14, 14)\n",
    "        \n",
    "        # Upsample về original input size\n",
    "        seg_map = F.interpolate(seg_map, size=original_size, mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Return logits (no sigmoid) for BCEWithLogitsLoss\n",
    "        return seg_map\n",
    "\n",
    "print(\"✅ ViT Segmentation model defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT Segmentation Model Evaluation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🔍 EVALUATING VIT SEGMENTATION MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if os.path.exists('models/unet_vit_model_best.pth'):\n",
    "    print('🔄 Loading ViT Segmentation model...')\n",
    "    \n",
    "    # Load ViT Segmentation (same as training)\n",
    "    vit_segmentation_model = ViTSegmentation(\n",
    "        vit_model_name=\"vit_tiny_patch16_224\",\n",
    "        num_classes=1,\n",
    "        pretrained=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load state dict\n",
    "    vit_segmentation_model.load_state_dict(torch.load('models/unet_vit_model_best.pth'))\n",
    "    vit_segmentation_model.eval()\n",
    "    \n",
    "    print('✅ ViT Segmentation model loaded successfully!')\n",
    "    \n",
    "    # Evaluate\n",
    "    vit_segmentation_results = evaluate_model(vit_segmentation_model, test_loader, 'ViT Segmentation')\n",
    "    \n",
    "    # Save results\n",
    "    with open('models/vit_segmentation_test_results.json', 'w') as f:\n",
    "        results_to_save = vit_segmentation_results.copy()\n",
    "        results_to_save['dice_scores'] = [float(x) for x in vit_segmentation_results['dice_scores']]\n",
    "        results_to_save['jaccard_scores'] = [float(x) for x in vit_segmentation_results['jaccard_scores']]\n",
    "        json.dump(results_to_save, f, indent=2)\n",
    "    \n",
    "    print('💾 ViT Segmentation results saved!')\n",
    "else:\n",
    "    print('❌ ViT Segmentation model not found. Please train it first.')\n",
    "    vit_segmentation_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Model 4: DeepLabV3+ ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepLabV3+ ResNet Model Evaluation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🔍 EVALUATING DEEPLABV3+ RESNET MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if os.path.exists('models/deeplabv3_resnet_model_best.pth'):\n",
    "    print('🔄 Loading DeepLabV3+ ResNet model...')\n",
    "    \n",
    "    # Load DeepLabV3+ ResNet (same as training)\n",
    "    deeplabv3_resnet_model = smp.DeepLabV3Plus(\n",
    "        encoder_name=\"resnet50\",\n",
    "        encoder_weights=\"imagenet\",\n",
    "        in_channels=3,\n",
    "        classes=1,\n",
    "        activation=None  # We handle sigmoid manually\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load state dict\n",
    "    deeplabv3_resnet_model.load_state_dict(torch.load('models/deeplabv3_resnet_model_best.pth'))\n",
    "    deeplabv3_resnet_model.eval()\n",
    "    \n",
    "    print('✅ DeepLabV3+ ResNet model loaded successfully!')\n",
    "    \n",
    "    # Evaluate\n",
    "    deeplabv3_resnet_results = evaluate_model(deeplabv3_resnet_model, test_loader, 'DeepLabV3+ ResNet')\n",
    "    \n",
    "    # Save results\n",
    "    with open('models/deeplabv3_resnet_test_results.json', 'w') as f:\n",
    "        results_to_save = deeplabv3_resnet_results.copy()\n",
    "        results_to_save['dice_scores'] = [float(x) for x in deeplabv3_resnet_results['dice_scores']]\n",
    "        results_to_save['jaccard_scores'] = [float(x) for x in deeplabv3_resnet_results['jaccard_scores']]\n",
    "        json.dump(results_to_save, f, indent=2)\n",
    "    \n",
    "    print('💾 DeepLabV3+ ResNet results saved!')\n",
    "else:\n",
    "    print('❌ DeepLabV3+ ResNet model not found. Please train it first.')\n",
    "    deeplabv3_resnet_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Results Comparison và Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "all_results = []\n",
    "\n",
    "# Load results from JSON files if they exist\n",
    "model_files = {\n",
    "    'SegFormer': 'models/segformer_test_results.json',\n",
    "    'U-Net EfficientNet': 'models/unet_efficientnet_test_results.json',\n",
    "    'ViT Segmentation': 'models/vit_segmentation_test_results.json',\n",
    "    'DeepLabV3+ ResNet': 'models/deeplabv3_resnet_test_results.json'\n",
    "}\n",
    "\n",
    "for model_name, file_path in model_files.items():\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            results = json.load(f)\n",
    "            all_results.append(results)\n",
    "        print(f'✅ Loaded {model_name} results')\n",
    "    else:\n",
    "        print(f'❌ {model_name} results not found')\n",
    "\n",
    "print(f'\\n📊 Total models evaluated: {len(all_results)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "if all_results:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📊 MODEL COMPARISON RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    comparison_data = []\n",
    "    for result in all_results:\n",
    "        comparison_data.append({\n",
    "            'Model': result['model_name'],\n",
    "            'Test Loss': f\"{result['test_loss']:.4f}\",\n",
    "            'Dice Score': f\"{result['dice_mean']:.4f} ± {result['dice_std']:.4f}\",\n",
    "            'Jaccard (IoU)': f\"{result['jaccard_mean']:.4f} ± {result['jaccard_std']:.4f}\",\n",
    "            'Samples': result['num_samples']\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    print(df_comparison.to_string(index=False))\n",
    "    \n",
    "    # Find best model\n",
    "    best_dice_idx = np.argmax([r['dice_mean'] for r in all_results])\n",
    "    best_jaccard_idx = np.argmax([r['jaccard_mean'] for r in all_results])\n",
    "    \n",
    "    print(f\"\\n🏆 BEST PERFORMANCE:\")\n",
    "    print(f\"   - Best Dice Score: {all_results[best_dice_idx]['model_name']} ({all_results[best_dice_idx]['dice_mean']:.4f})\")\n",
    "    print(f\"   - Best Jaccard (IoU): {all_results[best_jaccard_idx]['model_name']} ({all_results[best_jaccard_idx]['jaccard_mean']:.4f})\")\n",
    "    \n",
    "    # Save comparison table\n",
    "    df_comparison.to_csv('models/model_comparison_results.csv', index=False)\n",
    "    print(f\"\\n💾 Comparison table saved to: models/model_comparison_results.csv\")\n",
    "else:\n",
    "    print(\"❌ No results found for comparison. Please run model evaluations first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "if all_results:\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Dice Score Comparison\n",
    "    plt.subplot(2, 2, 1)\n",
    "    models = [r['model_name'] for r in all_results]\n",
    "    dice_means = [r['dice_mean'] for r in all_results]\n",
    "    dice_stds = [r['dice_std'] for r in all_results]\n",
    "    \n",
    "    bars = plt.bar(models, dice_means, yerr=dice_stds, capsize=5, alpha=0.7)\n",
    "    plt.title('Dice Score Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Dice Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, mean in zip(bars, dice_means):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{mean:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Jaccard (IoU) Comparison\n",
    "    plt.subplot(2, 2, 2)\n",
    "    jaccard_means = [r['jaccard_mean'] for r in all_results]\n",
    "    jaccard_stds = [r['jaccard_std'] for r in all_results]\n",
    "    \n",
    "    bars = plt.bar(models, jaccard_means, yerr=jaccard_stds, capsize=5, alpha=0.7, color='orange')\n",
    "    plt.title('Jaccard Index (IoU) Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Jaccard Index')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, mean in zip(bars, jaccard_means):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{mean:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Plot 3: Test Loss Comparison\n",
    "    plt.subplot(2, 2, 3)\n",
    "    test_losses = [r['test_loss'] for r in all_results]\n",
    "    \n",
    "    bars = plt.bar(models, test_losses, alpha=0.7, color='red')\n",
    "    plt.title('Test Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Test Loss')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, loss in zip(bars, test_losses):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{loss:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Plot 4: Dice Score Distribution (Box Plot)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    dice_distributions = [r['dice_scores'] for r in all_results]\n",
    "    \n",
    "    plt.boxplot(dice_distributions, labels=models)\n",
    "    plt.title('Dice Score Distribution', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Dice Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('models/model_comparison_plots.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"💾 Comparison plots saved to: models/model_comparison_plots.png\")\n",
    "else:\n",
    "    print(\"❌ No results available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Evaluation Complete!\n",
    "\n",
    "### 📁 Output Files:\n",
    "- **Individual Results**: `models/*_test_results.json`\n",
    "- **Comparison Table**: `models/model_comparison_results.csv`\n",
    "- **Visualization**: `models/model_comparison_plots.png`\n",
    "\n",
    "### 🔍 Next Steps:\n",
    "1. **Analyze Results**: Compare performance metrics\n",
    "2. **Select Best Model**: Based on Dice/IoU scores\n",
    "3. **Error Analysis**: Investigate failure cases\n",
    "4. **Model Ensemble**: Combine best performing models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
